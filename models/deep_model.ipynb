{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63e3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe27888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8763734",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07939271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#only 10 rows with missing data just dropping\n",
    "data = data[data['MISSING_DATA'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2774d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyline_to_trip_duration(polyline):\n",
    "    return max(polyline.count(\"[\") - 2, 0) * 15 #subtracting 2 because one is for the opening bracket?\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "data[\"LEN\"] = data[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9790a30",
   "metadata": {},
   "source": [
    "**Making time columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8332458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.year, dt.month, dt.day, dt.hour, dt.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc43300",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827c6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling nan's with -1 to avoid errors\n",
    "data['ORIGIN_STAND'].fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b7bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map = {'A': 1, 'B': 2, 'C': 3}\n",
    "data['CALL_TYPE'] = data['CALL_TYPE'].map(type_map).astype(int) \n",
    "data['DAY_TYPE'] = data[\"DAY_TYPE\"].map(type_map).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f130faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the data is very wide after onehotencoding, we'll do a 90-10 split to give the model more data\n",
    "train, valid = train_test_split(data, train_size = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d4a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data  #clearing memory since this dataframe is now redundant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5c61",
   "metadata": {},
   "source": [
    "**Add interaction terms** ?\n",
    "- day * hour\n",
    "- hour * origin stand\n",
    "- day_type * origin stand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c7631",
   "metadata": {},
   "source": [
    "## Data engineering\n",
    "- Should all the features be categorical? Seems wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b38ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND', 'CALL_TYPE']]\n",
    "Y_train = train[\"LEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af89deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = valid[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND', 'CALL_TYPE']] \n",
    "Y_valid = valid[\"LEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba704b6e",
   "metadata": {},
   "source": [
    "## Using one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104f2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehot encoding\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "encoder.fit(X_train[X_train.columns])\n",
    "X_train_OH = encoder.transform(X_train[X_train.columns])\n",
    "\n",
    "X_train = torch.Tensor(X_train_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f92e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_OH = encoder.transform(X_valid[X_valid.columns])\n",
    "\n",
    "X_valid_OH = torch.Tensor(X_valid_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e12ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = torch.utils.data.TensorDataset(X_train, torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "423f2da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size), #added batchnorm\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(100, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        # Pass through the encoder\n",
    "        output = self.encoder(data)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da7a50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DeeperModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 2*hidden_size), #added another hidden layer compared to writeup\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(2*hidden_size, hidden_size), #added another hidden layer compared to writeup\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size), #added batchnorm, is this correct?\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, 100),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        # Pass through the encoder\n",
    "        output = self.encoder(data)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96266937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1454061, 143])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a31701b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024 #large because of the onehot encoding, we need a wide model\n",
    "\n",
    "model = DeepModel(10296, #changed from X_train.size(1)\n",
    "                   hidden_size).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4) #can try 1e-4, similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dffccc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Time of epoch 1: 147.28156995773315 seconds\n",
      "Epoch 1 training loss: 680.489791690228\n",
      "-------------------------------\n",
      "Time of epoch 2: 140.23842406272888 seconds\n",
      "Epoch 2 training loss: 671.0932236633018\n",
      "-------------------------------\n",
      "Time of epoch 3: 156.88070607185364 seconds\n",
      "Epoch 3 training loss: 669.1003411932471\n",
      "-------------------------------\n",
      "Time of epoch 4: 141.5887234210968 seconds\n",
      "Epoch 4 training loss: 667.0408019176109\n",
      "-------------------------------\n",
      "Time of epoch 5: 153.47037768363953 seconds\n",
      "Epoch 5 training loss: 665.0471969341506\n"
     ]
    }
   ],
   "source": [
    "#deep network\n",
    "\n",
    "\n",
    "\n",
    "losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"-------------------------------\")\n",
    "    \n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        interactions = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "        x_batch = torch.Tensor(\n",
    "            interactions.fit_transform(x_batch.cpu())\n",
    "        )\n",
    "        \n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "    \n",
    "\n",
    "        y_batch_pred = model(x_batch)\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_batch_pred, y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Time of epoch {epoch + 1}: {time.time() - start_time} seconds')\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} training loss: {np.sqrt(running_loss/n)}\")\n",
    "    \n",
    "    #valid_epoch = model(X_valid_OH).detach().cpu().numpy()\n",
    "    #val_loss = rmse(valid_epoch, Y_valid)\n",
    "    #print(f'Validation loss: {val_loss}')\n",
    "    \n",
    "    losses.append(np.sqrt(running_loss/n))\n",
    "    #valid_losses.append(val_loss)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cc4583db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to stop loss from converging here? need a more complex model? \n",
    "    #A: one hot encoding makes the data sparse, so we need a much wider model. \n",
    "    #with this additional width, we need to make the model deeper as well in order to make sure\n",
    "    #it sees enough data\n",
    "#should we do a different encoding?\n",
    "\n",
    "#why is loss so different on validation vs test? which loss should we believe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7469ae4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "#plt.plot(valid_losses)\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('loss_by_epoch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5d412e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_OH = encoder.transform(X_valid[X_valid.columns])\n",
    "\n",
    "X_valid_OH = torch.Tensor(X_valid_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b6c6ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 315,  255,  195, ..., 1350,  750,  390])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e94aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664.5225"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds = model(X_valid_OH)\n",
    "np.sqrt(\n",
    "    F.mse_loss(val_preds.cpu(), \n",
    "               torch.Tensor(Y_valid.values)).detach().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aacfd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42068363477644105\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(val_preds.detach().cpu()).nunique()/len(val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad849f4",
   "metadata": {},
   "source": [
    "**Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d93c8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "LR: 0.5, size: 128\n",
      " Loss: 686.6440361774972\n",
      "-------------------------------\n",
      "LR: 0.5, size: 256\n",
      " Loss: 686.6473317833044\n",
      "-------------------------------\n",
      "LR: 0.5, size: 512\n",
      " Loss: 686.6414258090298\n",
      "-------------------------------\n",
      "LR: 0.5, size: 1024\n",
      " Loss: 686.6549456522384\n",
      "-------------------------------\n",
      "LR: 0.01, size: 128\n",
      " Loss: 673.459939999522\n",
      "-------------------------------\n",
      "LR: 0.01, size: 256\n",
      " Loss: 673.3501568113429\n",
      "-------------------------------\n",
      "LR: 0.01, size: 512\n",
      " Loss: 673.1819186666024\n",
      "-------------------------------\n",
      "LR: 0.01, size: 1024\n",
      " Loss: 672.9186133251183\n",
      "-------------------------------\n",
      "LR: 0.001, size: 128\n",
      " Loss: 672.9877717424525\n",
      "-------------------------------\n",
      "LR: 0.001, size: 256\n",
      " Loss: 672.8936539054798\n",
      "-------------------------------\n",
      "LR: 0.001, size: 512\n",
      " Loss: 672.7044547102497\n",
      "-------------------------------\n",
      "LR: 0.001, size: 1024\n",
      " Loss: 672.5458404702964\n",
      "-------------------------------\n",
      "LR: 0.0001, size: 128\n",
      " Loss: 676.012301706915\n",
      "-------------------------------\n",
      "LR: 0.0001, size: 256\n",
      " Loss: 675.2140971341825\n",
      "-------------------------------\n",
      "LR: 0.0001, size: 512\n",
      " Loss: 673.7615776833715\n",
      "-------------------------------\n",
      "LR: 0.0001, size: 1024\n",
      " Loss: 672.6688494041234\n",
      "-------------------------------\n",
      "LR: 1e-05, size: 128\n",
      " Loss: 681.6675236252534\n",
      "-------------------------------\n",
      "LR: 1e-05, size: 256\n",
      " Loss: 680.9513861243861\n",
      "-------------------------------\n",
      "LR: 1e-05, size: 512\n",
      " Loss: 680.3220178444067\n",
      "-------------------------------\n",
      "LR: 1e-05, size: 1024\n",
      " Loss: 679.4707795292112\n",
      "-------------------------------\n",
      "LR: 1e-06, size: 128\n",
      " Loss: 975.9474392617052\n",
      "-------------------------------\n",
      "LR: 1e-06, size: 256\n",
      " Loss: 936.8125458500687\n",
      "-------------------------------\n",
      "LR: 1e-06, size: 512\n",
      " Loss: 754.9081975859058\n",
      "-------------------------------\n",
      "LR: 1e-06, size: 1024\n",
      " Loss: 685.0890238995557\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [5e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "hidden_sizes = [128, 256, 512, 1024]\n",
    "#define model\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    for hidden_size in hidden_sizes:\n",
    "        \n",
    "        model = DeepModel(X_train.size(1),\n",
    "                           hidden_size).to(device)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "        \n",
    "        print(f\"-------------------------------\")\n",
    "        \n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(3):\n",
    "            start_time = time.time()\n",
    "\n",
    "            running_loss = 0\n",
    "            n = 0\n",
    "\n",
    "            # Loop over batches in an epoch using DataLoader\n",
    "            for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "\n",
    "                y_batch_pred = model(x_batch)\n",
    "\n",
    "\n",
    "                loss = loss_fn(y_batch_pred, y_batch.float())\n",
    "                running_loss += loss.item()\n",
    "                n += 1\n",
    "\n",
    "                # backwards steps\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #print(f'Time of epoch {epoch + 1}: {time.time() - start_time} seconds')\n",
    "            \n",
    "            if epoch == 2:\n",
    "                print(f\"LR: {lr}, size: {hidden_size}\\n Loss: {np.sqrt(running_loss/n)}\")\n",
    "                losses.append(np.sqrt(running_loss/n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2577530",
   "metadata": {},
   "source": [
    "### Analyzing trips which were hard to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not enough memory\n",
    "y_train_pred = model.cpu()(X_train.cpu())\n",
    "with_preds = train.copy()\n",
    "with_preds['diff'] = abs(train['LEN'] - y_train_pred)\n",
    "\n",
    "ten_worst = with_preds.sort_values(by = 'diff', ascending = False).iloc[:10][['POLYLINE', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polyline(polyline):\n",
    "    x = [point[0] for point in polyline]\n",
    "    y = [point[1] for point in polyline]\n",
    "\n",
    "    plt.plot(x, y, '-o')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Polyline Plot')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    plot_polyline(\n",
    "        eval(\n",
    "            ten_worst.reset_index()['POLYLINE'].iloc[i]\n",
    "        )\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451e0c3",
   "metadata": {},
   "source": [
    "### Prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8bb6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd640ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    test_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da628172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND', 'CALL_TYPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ced1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_OH = encoder.transform(X_test[X_test.columns])\n",
    "X_test_OH = interactions.transform(X_test_OH)\n",
    "\n",
    "X_test = torch.Tensor(X_test_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58a27989",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "717e4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': train['LEN'].mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d0198bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': preds.cpu().detach()})\n",
    "\n",
    "final_preds.to_csv('interaction_test_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380c2b8",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "**Not in use**  \n",
    "We still need to debug this, but it's our idea for how to improve the model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d021a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CALL_TYPE', 'ORIGIN_STAND', 'TIMESTAMP', 'YR', 'MON', 'DAY', 'HR',\n",
      "       'WK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, dropped_columns):\n",
    "        self.data = dataframe.drop(columns=dropped_columns)#.values\n",
    "        self.targets = dataframe[\"LEN\"].astype(float).values\n",
    "        print(self.data.columns)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print(self.data[idx])\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataset = CustomDataset(train, dropped_columns=['LEN', 'POLYLINE', \n",
    "                                                'MISSING_DATA', 'TRIP_ID', 'ORIGIN_CALL',\n",
    "                                               \"DAY_TYPE\", 'TAXI_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5069975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.targets = dataframe[\"LEN\"].astype(float).values\n",
    "        self.data = dataframe[['CALL_TYPE', 'ORIGIN_STAND', 'YR', 'MON', 'DAY', 'HR', 'WK']]\n",
    "        self.call_type_mapping = {'A': 0, 'B': 1, 'C': 2}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.iloc[idx, :].copy()\n",
    "        x['CALL_TYPE'] = self.call_type_mapping[x['CALL_TYPE']]\n",
    "        \n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "     \n",
    "        return torch.from_numpy(x.values.astype(np.float32)), y\n",
    "\n",
    "dataset = PandasDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d070c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70e5c6",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73160566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, embedding_dim, hidden_size):\n",
    "        super(EmbedModel, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, embedding_dim) for num_categories in embedding_sizes\n",
    "        ])\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(embedding_dim * len(embedding_sizes), hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        embedded_data = [\n",
    "            embedding_layer(data[:, i]) for i, embedding_layer in enumerate(self.embedding_layers)\n",
    "        ]\n",
    "        embedded_data = torch.cat(embedded_data, dim=1)\n",
    "        x = self.fc(embedded_data)\n",
    "        x = self.relu(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a4464b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_sizes = X_train.nunique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47948422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_665/1164054414.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['CALL_TYPE'] = X_train['CALL_TYPE'].map(call_map).astype(int)\n"
     ]
    }
   ],
   "source": [
    "call_map = {'A': 1, 'B': 2, 'C': 3}\n",
    "X_train['CALL_TYPE'] = X_train['CALL_TYPE'].map(call_map).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99cc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3d95a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataset = torch.utils.data.TensorDataset(X_train, torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d81530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbedModel(embedding_sizes, \n",
    "                   64, \n",
    "                   100).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdda2e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [12,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [21,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [9,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_665/411215713.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_665/2507890916.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     16\u001b[0m         ]\n\u001b[1;32m     17\u001b[0m         \u001b[0membedded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"-------------------------------\")\n",
    "    \n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device).long()\n",
    "        y_batch = y_batch.to(device)\n",
    "    \n",
    "\n",
    "        y_batch_pred = model(x_batch)\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_batch_pred, y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Time of epoch {epoch + 1}: {time.time() - start_time} seconds')\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} loss: {np.sqrt(running_loss/n)}\")\n",
    "    losses.append(np.sqrt(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96546ae4",
   "metadata": {},
   "source": [
    "**Older model**\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f9dc999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, num_call_types, num_origin_stands, embedding_dim, hidden_size):\n",
    "        super(BasicModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.call_type_embedding = nn.Embedding(num_call_types, embedding_dim)\n",
    "        self.origin_stand_embedding = nn.Embedding(num_origin_stands, embedding_dim)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(24, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, call_type, ):\n",
    "        call_type_embedded = self.call_type_embedding(call_type)\n",
    "        origin_stand_embedded = self.origin_stand_embedding(origin_stand)\n",
    "\n",
    "        hour = hour.unsqueeze(1)\n",
    "        day = day.unsqueeze(1)\n",
    "        month = month.unsqueeze(1)\n",
    "        year = year.unsqueeze(1)\n",
    "\n",
    "        # Concatenating\n",
    "        features = torch.cat([hour, day, month, year, call_type_embedded, origin_stand_embedded], dim=1)\n",
    "\n",
    "        output = self.encoder(features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "faee631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure about input size\n",
    "num_call_types = train['CALL_TYPE'].nunique() \n",
    "num_origin_stands = train['ORIGIN_STAND'].nunique() \n",
    "embedding_dim = 10 #arbitrary\n",
    "hidden_size = 64 #arbitrary\n",
    "\n",
    "model = BasicModel(num_call_types, \n",
    "                   num_origin_stands, embedding_dim, \n",
    "                   hidden_size).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac343b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85/831059653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Loop over batches in an epoch using DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#deep network (not lstm)\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        call_type = x_batch[:, 0].long()\n",
    "        origin_stand = x_batch[:, 1].long()\n",
    "        year = x_batch[:, 2]  \n",
    "        month = x_batch[:, 3]  \n",
    "        day = x_batch[:, 4]\n",
    "        hour = x_batch[:, 5]\n",
    "        week = x_batch[:, 6]\n",
    "        \n",
    "        min_origin_stand = torch.min(origin_stand)\n",
    "        origin_stand_processed = origin_stand - min_origin_stand\n",
    "\n",
    "        y_batch_pred = model(hour, day, month, year, call_type, origin_stand_processed)\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_batch_pred[0], y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} loss: {running_loss/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebeb43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 1 loss: 474716.41855863854\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 2 loss: 474706.96882588905\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 3 loss: 474695.4468317422\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 4 loss: 474669.8388911181\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 5 loss: 474651.3308392597\n"
     ]
    }
   ],
   "source": [
    "hidden_state = (torch.zeros(1, hidden_size).to(device),\n",
    "                torch.zeros(1, hidden_size).to(device))\n",
    "\n",
    "# https://stackoverflow.com/questions/51735001/how-to-include-batch-size-in-pytorch-basic-example\n",
    "for epoch in range(5):\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_batch_pred = model(x_batch, hidden_state)\n",
    "\n",
    "        loss = loss_fn(y_batch_pred[0], y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} loss: {running_loss/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53a492c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = valid[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "\n",
    "X_valid_OH = encoder.transform(X_valid[X_valid.columns])\n",
    "\n",
    "X_valid = torch.Tensor(X_valid_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe6ec6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = (torch.zeros(1, hidden_size).to(device),\n",
    "                torch.zeros(1, hidden_size).to(device))\n",
    "valid_preds = model(X_valid)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "649f2786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[711.6207],\n",
       "        [708.5093],\n",
       "        [714.1364],\n",
       "        ...,\n",
       "        [714.6372],\n",
       "        [719.8740],\n",
       "        [717.0795]], device='cuda:0', grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e663f",
   "metadata": {},
   "source": [
    "## Prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3993752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc00fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    test_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1edbdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842e0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_OH = encoder.transform(X_test[X_test.columns])\n",
    "\n",
    "X_test = torch.Tensor(X_test_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95fee367",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(X_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b455519",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': train['LEN'].mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67208289",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': preds.cpu().detach()})\n",
    "\n",
    "final_preds.to_csv('test_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab6213",
   "metadata": {},
   "source": [
    "# Old code, can ignore\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31733b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263863"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction[train_fraction['ORIGIN_STAND'].isna()].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2464fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = train.sample(500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50de5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_fraction[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "y = train_fraction['LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ba5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_Y = torch.tensor(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d54f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X = torch.from_numpy(X.values.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80160ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.nn.Sequential(\n",
    "torch.nn.Linear(6, 1))\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer (this will perform your parameter updates use)\n",
    "lr = 5e-3\n",
    "opt = torch.optim.SGD(model_1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fdadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train.sample(500_000)\n",
    "X_test_pd = test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "y_test_pd = test['LEN']\n",
    "\n",
    "y_test = torch.tensor(y_test_pd.values)\n",
    "X_test = torch.tensor(X_test_pd.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orenciolli/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([500000])) that is different to the input size (torch.Size([500000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "train_err = []\n",
    "test_err = []\n",
    "parameters = []\n",
    "for i in range(100):\n",
    "    model_1.train()\n",
    "  \n",
    "    y_pred = model_1(full_X) # Compute model outputs\n",
    "    loss = loss_fn(y_pred, full_Y) # Compute MSE\n",
    "    opt.zero_grad() # Must reset the gradients every step. Otherwise, gradients from previous iterations would cause interference!!!\n",
    "    loss.backward() # Compute gradients of all parameters (our model) with respect to our computed loss value (a singular value)\n",
    "    opt.step() # One gradient step\n",
    "  \n",
    "    train_err.append(loss.item())\n",
    "  \n",
    "    model_1.eval()\n",
    "    with torch.no_grad():\n",
    "        test_err.append(loss_fn(model_1(X_test), y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b674c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "model_1(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cecf3",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6237bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fraction['POLYLINE'] = train_fraction['POLYLINE'].apply(lambda x: np.array(eval(x)).astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692e88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fraction.groupby('ORIGIN_STAND').mean(numeric_only = True).plot(kind = 'bar', y = 'TRIP_LENGTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3907e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['ORIGIN_STAND'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca3d86",
   "metadata": {},
   "source": [
    "Predicted important variables: polyline, interaction between timestamp and origin stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843ee5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    train_fraction[['CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TIMESTAMP', 'DAY_TYPE', 'POLYLINE']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7482dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826e758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_x = train_fraction['POLYLINE'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "025d0444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['POLYLINE'].iloc[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d770a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/8n__h2151hd3n3gtfckwb75w0000gn/T/ipykernel_28498/3306076192.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_type = np.array([train_fraction['POLYLINE'].iloc[0], train_fraction['POLYLINE'].iloc[1]])\n"
     ]
    }
   ],
   "source": [
    "test_type = np.array([train_fraction['POLYLINE'].iloc[0], train_fraction['POLYLINE'].iloc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f95381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    np.array([0.5, 1.0, 2.0], dtype=np.float16),\n",
    "    np.array([4.0, 6.0, 8.0], dtype=np.float16),\n",
    "])\n",
    "\n",
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c630dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.59509"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_fraction['POLYLINE'])[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eea24aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d = np.array([\n",
    "    np.array([\n",
    "        np.array([1, 2]),\n",
    "        np.array([2, 3])\n",
    "    ]),\n",
    "    np.array([\n",
    "        np.array([1, 2]),\n",
    "        np.array([2, 3])\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb89d8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_d.shape#[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88d25ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.59509 , 41.146434])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['POLYLINE'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5b8cac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [2, 3]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(three_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abebfde0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOLYLINE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.from_numpy(train_fraction['POLYLINE'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a9c78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOLYLINE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.tensor(train_fraction['POLYLINE'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
