{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53b93be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f0c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8763734",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09701ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07939271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#only 10 rows with missing data just dropping\n",
    "data = data[data['MISSING_DATA'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a08efa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe keep missing? just split into different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2774d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyline_to_trip_duration(polyline):\n",
    "    return max(polyline.count(\"[\") - 2, 0) * 15 #subtracting 2 because one is for the opening bracket?\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "data[\"LEN\"] = data[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9790a30",
   "metadata": {},
   "source": [
    "**Making time columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8332458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.year, dt.month, dt.day, dt.hour, dt.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcc43300",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6900/63328864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TIMESTAMP\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"expand\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9566\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9567\u001b[0m         )\n\u001b[0;32m-> 9568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9570\u001b[0m     def applymap(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mResType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mwrap_results\u001b[0;34m(self, results, res_index)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;31m# see if we can infer the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_results_for_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# dict of scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mwrap_results_for_axis\u001b[0;34m(self, results, res_index)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;31m# we have requested to expand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"expand\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_to_same_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;31m# we have a non-series and don't want inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36minfer_to_same_shape\u001b[0;34m(self, results, res_index)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_to_same_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mResType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;34m\"\"\"infer the results to the same shape as the input object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# _homogenize ensures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#  - all(len(x) == len(index) for x in arrays)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             val = sanitize_array(\n\u001b[0m\u001b[1;32m    618\u001b[0m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_convert_platform\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_object_array_from_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# The caller is responsible for ensuring that we have np.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_object_array_from_listlike\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[0;31m# numpy will try to interpret nested lists as further dimensions, hence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m     \u001b[0;31m# making a 1D array that contains list-likes is a bit tricky:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66c3c5",
   "metadata": {},
   "source": [
    "**Data engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling nan's with -1 to avoid errors\n",
    "data['ORIGIN_STAND'].fillna(-1, inplace = True)\n",
    "data['ORIGIN_CALL'].fillna(-1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map = {'A': 1, 'B': 2, 'C': 3}\n",
    "data['CALL_TYPE'] = data['CALL_TYPE'].map(type_map).astype(int) \n",
    "data['DAY_TYPE'] = data[\"DAY_TYPE\"].map(type_map).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactions between (day, hour), (hour, stand)\n",
    "#new features: \n",
    "#   TOD: [morning, afternoon, evening, late night]\n",
    "#   Time from noon (abs or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making interaction features\n",
    "data['STAND_HR'] = data['ORIGIN_STAND'].astype(str) + '_' + data['HR'].astype(str)\n",
    "data['STAND_DAY'] = data['DAY'].astype(str) + '_' + data['ORIGIN_STAND'].astype(str)\n",
    "data['DAY_HR'] = data['DAY'].astype(str) + '_' + data['HR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec07514",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TOD'] = pd.cut(data['HR'], bins=[0, 12, 16, 20, 24], labels=[1, 2, 3, 4], right=False)\n",
    "data['FROM_NOON'] = data['HR'] - 12 #note that this column is continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols = (['YR', 'MON', 'DAY', 'HR', 'WK',\n",
    "               'ORIGIN_STAND', 'CALL_TYPE', 'TAXI_ID',\n",
    "              'STAND_HR', 'STAND_DAY', 'DAY_HR', \"TOD\"])\n",
    "\n",
    "for column in embed_cols:\n",
    "    data[column] = data[column].astype('category').cat.codes\n",
    "\n",
    "#save this data as csv for later\n",
    "data.to_csv('engineered_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fe302",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270f85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5841be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the data is very wide, we'll do a 90-10 split to give the model more data\n",
    "train, valid = train_test_split(data, train_size = 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f199712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data  #clearing memory since this dataframe is now redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4849ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_cols = (['YR', 'MON', 'DAY', 'HR', 'WK',\n",
    "               'ORIGIN_STAND', 'CALL_TYPE', 'TAXI_ID',\n",
    "              'STAND_HR', 'STAND_DAY', 'DAY_HR', \"TOD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7b41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train[embed_cols].nunique().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d9261",
   "metadata": {},
   "source": [
    "**Preparing input for model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \n",
    "                 'ORIGIN_STAND', 'CALL_TYPE', \n",
    "                 'TAXI_ID', 'STAND_HR', 'STAND_DAY', \n",
    "                 'DAY_HR', \"TOD\"]]\n",
    "Y_train = train[\"LEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af89deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = valid[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND', \n",
    "                 'CALL_TYPE', 'TAXI_ID', 'STAND_HR',\n",
    "                 'STAND_DAY', 'DAY_HR', \"TOD\"]] \n",
    "Y_valid = valid[\"LEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "881d140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train.to_numpy())\n",
    "\n",
    "X_valid = torch.Tensor(X_valid.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "545becc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = torch.utils.data.TensorDataset(X_train, torch.from_numpy(Y_train.values))\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a15b370",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_775/342979233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee144afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95368116",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "addd0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Create the embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size//2, hidden_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "           \n",
    "            nn.Linear(hidden_size//4, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        embedded_data = self.embedding(data)\n",
    "        embedded_data = embedded_data.mean(dim=1)  # Aggregate embeddings (e.g., using mean)\n",
    "        output = self.encoder(embedded_data)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3daf151",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a52007",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = [64, 128, 256, 512]\n",
    "embedding_sizes = [64, 128, 256, 512, 1024]\n",
    "lrs = [5e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "batches = [32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cbc8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 163.46613121032715 seconds\n",
      "Epoch 1 training loss: 688.5153388285004\n",
      "Time of epoch 2: 166.93063712120056 seconds\n",
      "Epoch 2 training loss: 684.9085215116395\n",
      "Time of epoch 3: 168.03568387031555 seconds\n",
      "Epoch 3 training loss: 684.9100443872925\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 163.581481218338 seconds\n",
      "Epoch 1 training loss: 688.6728305730365\n",
      "Time of epoch 2: 161.67039704322815 seconds\n",
      "Epoch 2 training loss: 674.649149009022\n",
      "Time of epoch 3: 158.57069945335388 seconds\n",
      "Epoch 3 training loss: 669.3583506677844\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 157.11408686637878 seconds\n",
      "Epoch 1 training loss: 696.4648921587177\n",
      "Time of epoch 2: 156.9175627231598 seconds\n",
      "Epoch 2 training loss: 687.9038745503938\n",
      "Time of epoch 3: 162.60087656974792 seconds\n",
      "Epoch 3 training loss: 682.6051041871542\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 164.28265500068665 seconds\n",
      "Epoch 1 training loss: 704.1937469638377\n",
      "Time of epoch 2: 160.4145495891571 seconds\n",
      "Epoch 2 training loss: 694.6531909017795\n",
      "Time of epoch 3: 164.53130292892456 seconds\n",
      "Epoch 3 training loss: 692.5885985633306\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 155.60241532325745 seconds\n",
      "Epoch 1 training loss: 787.1027758385422\n",
      "Time of epoch 2: 135.64890384674072 seconds\n",
      "Epoch 2 training loss: 706.1050415961176\n",
      "Time of epoch 3: 152.70853757858276 seconds\n",
      "Epoch 3 training loss: 703.739302058441\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 159.31108736991882 seconds\n",
      "Epoch 1 training loss: 690.3202871665886\n",
      "Time of epoch 2: 173.1869659423828 seconds\n",
      "Epoch 2 training loss: 684.9073724233493\n",
      "Time of epoch 3: 172.4814715385437 seconds\n",
      "Epoch 3 training loss: 684.9075751787918\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 171.33910036087036 seconds\n",
      "Epoch 1 training loss: 687.3488812494824\n",
      "Time of epoch 2: 174.1741659641266 seconds\n",
      "Epoch 2 training loss: 675.3298116876479\n",
      "Time of epoch 3: 168.8472535610199 seconds\n",
      "Epoch 3 training loss: 669.7092790693969\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 165.40642619132996 seconds\n",
      "Epoch 1 training loss: 695.9275188675713\n",
      "Time of epoch 2: 166.4333517551422 seconds\n",
      "Epoch 2 training loss: 686.2072802871779\n",
      "Time of epoch 3: 165.7107503414154 seconds\n",
      "Epoch 3 training loss: 682.4783608148032\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 165.22699809074402 seconds\n",
      "Epoch 1 training loss: 790.7102447057521\n",
      "Time of epoch 2: 164.17425084114075 seconds\n",
      "Epoch 2 training loss: 781.8759811748808\n",
      "Time of epoch 3: 168.62057304382324 seconds\n",
      "Epoch 3 training loss: 779.3226336674502\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 165.2193694114685 seconds\n",
      "Epoch 1 training loss: 829.5129925010787\n",
      "Time of epoch 2: 165.98668813705444 seconds\n",
      "Epoch 2 training loss: 747.6773724673758\n",
      "Time of epoch 3: 169.30929946899414 seconds\n",
      "Epoch 3 training loss: 745.721474704173\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 159.9434814453125 seconds\n",
      "Epoch 1 training loss: 691.3099476405522\n",
      "Time of epoch 2: 163.24546122550964 seconds\n",
      "Epoch 2 training loss: 684.9120814549186\n",
      "Time of epoch 3: 162.64008498191833 seconds\n",
      "Epoch 3 training loss: 684.9095702229118\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 163.18161487579346 seconds\n",
      "Epoch 1 training loss: 689.574135302504\n",
      "Time of epoch 2: 165.9036602973938 seconds\n",
      "Epoch 2 training loss: 674.4221496782991\n",
      "Time of epoch 3: 164.8707194328308 seconds\n",
      "Epoch 3 training loss: 670.0404126429574\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 165.3218550682068 seconds\n",
      "Epoch 1 training loss: 703.5955005433547\n",
      "Time of epoch 2: 162.47692823410034 seconds\n",
      "Epoch 2 training loss: 689.2738952055653\n",
      "Time of epoch 3: 165.85826063156128 seconds\n",
      "Epoch 3 training loss: 684.03257061009\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 162.90798377990723 seconds\n",
      "Epoch 1 training loss: 705.7858490190049\n",
      "Time of epoch 2: 162.87138414382935 seconds\n",
      "Epoch 2 training loss: 696.3203322359009\n",
      "Time of epoch 3: 165.89546537399292 seconds\n",
      "Epoch 3 training loss: 694.3966574721946\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 164.86347603797913 seconds\n",
      "Epoch 1 training loss: 753.9953067267728\n",
      "Time of epoch 2: 161.78457760810852 seconds\n",
      "Epoch 2 training loss: 698.0913888102183\n",
      "Time of epoch 3: 163.97958517074585 seconds\n",
      "Epoch 3 training loss: 696.2035634203861\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 160.4604914188385 seconds\n",
      "Epoch 1 training loss: 689.2711652241109\n",
      "Time of epoch 2: 163.8488359451294 seconds\n",
      "Epoch 2 training loss: 684.9111358547339\n",
      "Time of epoch 3: 160.82089734077454 seconds\n",
      "Epoch 3 training loss: 684.9093927326423\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 160.7895061969757 seconds\n",
      "Epoch 1 training loss: 694.6208205182159\n",
      "Time of epoch 2: 160.552729845047 seconds\n",
      "Epoch 2 training loss: 673.8755013172208\n",
      "Time of epoch 3: 162.35363340377808 seconds\n",
      "Epoch 3 training loss: 672.8758409750756\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 163.32630467414856 seconds\n",
      "Epoch 1 training loss: 694.6653849939378\n",
      "Time of epoch 2: 164.01727175712585 seconds\n",
      "Epoch 2 training loss: 684.562022949755\n",
      "Time of epoch 3: 161.21414709091187 seconds\n",
      "Epoch 3 training loss: 680.0481977012324\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 512, lr: 0.0001\n",
      "Time of epoch 2: 160.8777048587799 seconds\n",
      "Epoch 2 training loss: 690.7400860593469\n",
      "Time of epoch 3: 160.9072380065918 seconds\n",
      "Epoch 3 training loss: 689.0714549194479\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 165.67642521858215 seconds\n",
      "Epoch 1 training loss: 755.3261788449761\n",
      "Time of epoch 2: 162.4450523853302 seconds\n",
      "Epoch 2 training loss: 701.4569294595706\n",
      "Time of epoch 3: 162.19736289978027 seconds\n",
      "Epoch 3 training loss: 699.191204914188\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 203.3223361968994 seconds\n",
      "Epoch 1 training loss: 695.0097812367602\n",
      "Time of epoch 2: 203.4984221458435 seconds\n",
      "Epoch 2 training loss: 684.9109136117514\n",
      "Time of epoch 3: 205.2254023551941 seconds\n",
      "Epoch 3 training loss: 684.912577722182\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 1024, lr: 0.01\n",
      "Time of epoch 2: 205.86556553840637 seconds\n",
      "Epoch 2 training loss: 678.3993304383838\n",
      "Time of epoch 3: 202.89339661598206 seconds\n",
      "Epoch 3 training loss: 675.3177683145736\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 204.11011123657227 seconds\n",
      "Epoch 1 training loss: 695.0813035707\n",
      "Time of epoch 2: 204.0483913421631 seconds\n",
      "Epoch 2 training loss: 685.2332213096606\n",
      "Time of epoch 3: 205.03319787979126 seconds\n",
      "Epoch 3 training loss: 678.5734668900272\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 206.65173244476318 seconds\n",
      "Epoch 1 training loss: 728.5046147860995\n",
      "Time of epoch 2: 203.28780817985535 seconds\n",
      "Epoch 2 training loss: 720.7081640567488\n",
      "Time of epoch 3: 203.03956031799316 seconds\n",
      "Epoch 3 training loss: 717.036828984147\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 64, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 204.85403537750244 seconds\n",
      "Epoch 1 training loss: 755.0633533670392\n",
      "Time of epoch 2: 205.86792469024658 seconds\n",
      "Epoch 2 training loss: 707.139936694719\n",
      "Time of epoch 3: 204.79145288467407 seconds\n",
      "Epoch 3 training loss: 705.1715342018256\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 158.97323155403137 seconds\n",
      "Epoch 1 training loss: 703.2795257359926\n",
      "Time of epoch 2: 160.45157837867737 seconds\n",
      "Epoch 2 training loss: 684.9092252546817\n",
      "Time of epoch 3: 163.25681161880493 seconds\n",
      "Epoch 3 training loss: 684.9271881231025\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 160.02449584007263 seconds\n",
      "Epoch 1 training loss: 678.8093918671333\n",
      "Time of epoch 2: 161.2837233543396 seconds\n",
      "Epoch 2 training loss: 670.7975224764712\n",
      "Time of epoch 3: 163.18537545204163 seconds\n",
      "Epoch 3 training loss: 669.2089204065626\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 161.37024521827698 seconds\n",
      "Epoch 1 training loss: 687.9529429029196\n",
      "Time of epoch 2: 158.6552209854126 seconds\n",
      "Epoch 2 training loss: 678.0974152524228\n",
      "Time of epoch 3: 158.4555823802948 seconds\n",
      "Epoch 3 training loss: 673.8391222550028\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 159.4303424358368 seconds\n",
      "Epoch 1 training loss: 696.9417620037046\n",
      "Time of epoch 2: 159.39542937278748 seconds\n",
      "Epoch 2 training loss: 688.1212465482076\n",
      "Time of epoch 3: 155.07041668891907 seconds\n",
      "Epoch 3 training loss: 686.064672352209\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 157.69267916679382 seconds\n",
      "Epoch 1 training loss: 745.7463584521978\n",
      "Time of epoch 2: 158.6745891571045 seconds\n",
      "Epoch 2 training loss: 693.5526469178991\n",
      "Time of epoch 3: 156.10877418518066 seconds\n",
      "Epoch 3 training loss: 691.368195946664\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 161.11199069023132 seconds\n",
      "Epoch 1 training loss: 736.5781538799876\n",
      "Time of epoch 2: 156.9643440246582 seconds\n",
      "Epoch 2 training loss: 684.9081553215378\n",
      "Time of epoch 3: 155.94874668121338 seconds\n",
      "Epoch 3 training loss: 684.9047873362205\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 159.4892177581787 seconds\n",
      "Epoch 1 training loss: 680.1335940103039\n",
      "Time of epoch 2: 159.91849780082703 seconds\n",
      "Epoch 2 training loss: 670.9868315195412\n",
      "Time of epoch 3: 146.66682887077332 seconds\n",
      "Epoch 3 training loss: 669.9997075224104\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 139.9424605369568 seconds\n",
      "Epoch 1 training loss: 685.368046668771\n",
      "Time of epoch 2: 138.32830142974854 seconds\n",
      "Epoch 2 training loss: 676.281846443057\n",
      "Time of epoch 3: 138.97062277793884 seconds\n",
      "Epoch 3 training loss: 672.2131389524786\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 139.97656059265137 seconds\n",
      "Epoch 1 training loss: 695.6173735403748\n",
      "Time of epoch 2: 150.38167667388916 seconds\n",
      "Epoch 2 training loss: 687.3757116602233\n",
      "Time of epoch 3: 166.17334532737732 seconds\n",
      "Epoch 3 training loss: 685.612677035443\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 160.1633815765381 seconds\n",
      "Epoch 1 training loss: 731.4787713419422\n",
      "Time of epoch 2: 161.59591555595398 seconds\n",
      "Epoch 2 training loss: 691.0487402433928\n",
      "Time of epoch 3: 151.06395316123962 seconds\n",
      "Epoch 3 training loss: 689.0211724934484\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 152.30151224136353 seconds\n",
      "Epoch 1 training loss: 713.8657538072036\n",
      "Time of epoch 2: 156.67145323753357 seconds\n",
      "Epoch 2 training loss: 684.9133294234396\n",
      "Time of epoch 3: 151.97795510292053 seconds\n",
      "Epoch 3 training loss: 684.9092375977086\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 161.51285314559937 seconds\n",
      "Epoch 1 training loss: 679.3180697798019\n",
      "Time of epoch 2: 161.55279874801636 seconds\n",
      "Epoch 2 training loss: 672.3410109874002\n",
      "Time of epoch 3: 165.2208640575409 seconds\n",
      "Epoch 3 training loss: 674.1040308663545\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 164.68441486358643 seconds\n",
      "Epoch 1 training loss: 684.2297529248891\n",
      "Time of epoch 2: 162.55905508995056 seconds\n",
      "Epoch 2 training loss: 674.8580016027728\n",
      "Time of epoch 3: 164.38434982299805 seconds\n",
      "Epoch 3 training loss: 671.2222979958088\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 163.924063205719 seconds\n",
      "Epoch 1 training loss: 688.5713841146535\n",
      "Time of epoch 2: 172.5699439048767 seconds\n",
      "Epoch 2 training loss: 681.2465096360468\n",
      "Time of epoch 3: 171.85830736160278 seconds\n",
      "Epoch 3 training loss: 679.171536125395\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 170.73756957054138 seconds\n",
      "Epoch 1 training loss: 726.7620716031207\n",
      "Time of epoch 2: 170.03892278671265 seconds\n",
      "Epoch 2 training loss: 690.0295119052402\n",
      "Time of epoch 3: 169.25019454956055 seconds\n",
      "Epoch 3 training loss: 687.7881424723436\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 164.75018191337585 seconds\n",
      "Epoch 1 training loss: 906.0490149866296\n",
      "Time of epoch 2: 164.95944118499756 seconds\n",
      "Epoch 2 training loss: 684.9111317177322\n",
      "Time of epoch 3: 167.18086647987366 seconds\n",
      "Epoch 3 training loss: 684.9079903789662\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 165.23346281051636 seconds\n",
      "Epoch 1 training loss: 682.062166306672\n",
      "Time of epoch 2: 160.92934608459473 seconds\n",
      "Epoch 2 training loss: 675.923157787872\n",
      "Time of epoch 3: 165.58330845832825 seconds\n",
      "Epoch 3 training loss: 673.5887789910178\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 161.22864079475403 seconds\n",
      "Epoch 1 training loss: 682.807069265281\n",
      "Time of epoch 2: 166.32255697250366 seconds\n",
      "Epoch 2 training loss: 673.1328340522142\n",
      "Time of epoch 3: 163.15955138206482 seconds\n",
      "Epoch 3 training loss: 669.4278089225603\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 160.4208266735077 seconds\n",
      "Epoch 1 training loss: 689.495297137984\n",
      "Time of epoch 2: 162.4587368965149 seconds\n",
      "Epoch 2 training loss: 681.3151023811774\n",
      "Time of epoch 3: 161.88308572769165 seconds\n",
      "Epoch 3 training loss: 678.1677575484556\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 161.44992208480835 seconds\n",
      "Epoch 1 training loss: 726.7749446893065\n",
      "Time of epoch 2: 160.48105812072754 seconds\n",
      "Epoch 2 training loss: 693.8975817262285\n",
      "Time of epoch 3: 161.51781511306763 seconds\n",
      "Epoch 3 training loss: 692.3309641560666\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 207.22907328605652 seconds\n",
      "Epoch 1 training loss: 1055.1371842949784\n",
      "Time of epoch 2: 205.3983371257782 seconds\n",
      "Epoch 2 training loss: 684.9093487836257\n",
      "Time of epoch 3: 206.61992812156677 seconds\n",
      "Epoch 3 training loss: 684.9124437417912\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 204.41338658332825 seconds\n",
      "Epoch 1 training loss: 680.4010691018683\n",
      "Time of epoch 2: 205.50528645515442 seconds\n",
      "Epoch 2 training loss: 678.3200256371099\n",
      "Time of epoch 3: 203.6488389968872 seconds\n",
      "Epoch 3 training loss: 674.8720667110275\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 205.67806959152222 seconds\n",
      "Epoch 1 training loss: 684.3020573158822\n",
      "Time of epoch 2: 205.40658736228943 seconds\n",
      "Epoch 2 training loss: 674.2550431882349\n",
      "Time of epoch 3: 204.4011607170105 seconds\n",
      "Epoch 3 training loss: 669.253717486235\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 204.57114386558533 seconds\n",
      "Epoch 1 training loss: 690.8764511963118\n",
      "Time of epoch 2: 206.25215482711792 seconds\n",
      "Epoch 2 training loss: 682.260614018125\n",
      "Time of epoch 3: 205.9251229763031 seconds\n",
      "Epoch 3 training loss: 679.0660624570033\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 128, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 203.66904044151306 seconds\n",
      "Epoch 1 training loss: 726.1200595973274\n",
      "Time of epoch 2: 205.11903834342957 seconds\n",
      "Epoch 2 training loss: 696.2748478333847\n",
      "Time of epoch 3: 203.57774782180786 seconds\n",
      "Epoch 3 training loss: 694.5524518349257\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 164.22790718078613 seconds\n",
      "Epoch 1 training loss: 788.2940754631944\n",
      "Time of epoch 2: 164.7865092754364 seconds\n",
      "Epoch 2 training loss: 684.977408517296\n",
      "Time of epoch 3: 159.5230541229248 seconds\n",
      "Epoch 3 training loss: 684.9116776124177\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 157.20963168144226 seconds\n",
      "Epoch 1 training loss: 675.6214744000863\n",
      "Time of epoch 2: 161.32193040847778 seconds\n",
      "Epoch 2 training loss: 669.8632723439086\n",
      "Time of epoch 3: 163.83200216293335 seconds\n",
      "Epoch 3 training loss: 669.3022738850606\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 160.67534494400024 seconds\n",
      "Epoch 1 training loss: 676.3780819456573\n",
      "Time of epoch 2: 161.78237080574036 seconds\n",
      "Epoch 2 training loss: 667.0389944484175\n",
      "Time of epoch 3: 157.81823205947876 seconds\n",
      "Epoch 3 training loss: 663.6273952250843\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 162.31156945228577 seconds\n",
      "Epoch 1 training loss: 688.1821875011617\n",
      "Time of epoch 2: 161.9091227054596 seconds\n",
      "Epoch 2 training loss: 680.7286376726305\n",
      "Time of epoch 3: 157.74793028831482 seconds\n",
      "Epoch 3 training loss: 678.731783794571\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 161.00410962104797 seconds\n",
      "Epoch 1 training loss: 720.2568702726029\n",
      "Time of epoch 2: 161.6565818786621 seconds\n",
      "Epoch 2 training loss: 689.7862352844459\n",
      "Time of epoch 3: 159.72251152992249 seconds\n",
      "Epoch 3 training loss: 687.6737560331849\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 160.20614504814148 seconds\n",
      "Epoch 1 training loss: 1310.2597855284632\n",
      "Time of epoch 2: 160.80109286308289 seconds\n",
      "Epoch 2 training loss: 684.9039163849668\n",
      "Time of epoch 3: 159.79456520080566 seconds\n",
      "Epoch 3 training loss: 684.9186025155146\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 158.66038179397583 seconds\n",
      "Epoch 1 training loss: 676.274865696365\n",
      "Time of epoch 2: 158.2261962890625 seconds\n",
      "Epoch 2 training loss: 671.6433716755294\n",
      "Time of epoch 3: 160.02395057678223 seconds\n",
      "Epoch 3 training loss: 669.5760466585585\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 162.75582766532898 seconds\n",
      "Epoch 1 training loss: 676.3953649522772\n",
      "Time of epoch 2: 159.7259464263916 seconds\n",
      "Epoch 2 training loss: 666.9308766479851\n",
      "Time of epoch 3: 156.46819019317627 seconds\n",
      "Epoch 3 training loss: 662.7084618571711\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 162.21165370941162 seconds\n",
      "Epoch 1 training loss: 685.6426016483316\n",
      "Time of epoch 2: 157.05812525749207 seconds\n",
      "Epoch 2 training loss: 678.4131378446383\n",
      "Time of epoch 3: 162.08898520469666 seconds\n",
      "Epoch 3 training loss: 676.0492677057177\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 154.54836893081665 seconds\n",
      "Epoch 1 training loss: 713.8970643421644\n",
      "Time of epoch 2: 160.37717056274414 seconds\n",
      "Epoch 2 training loss: 687.4471587617667\n",
      "Time of epoch 3: 158.03543186187744 seconds\n",
      "Epoch 3 training loss: 685.7204493322465\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 161.63855004310608 seconds\n",
      "Epoch 1 training loss: 2932.60522473354\n",
      "Time of epoch 2: 158.7782073020935 seconds\n",
      "Epoch 2 training loss: 684.9390775122223\n",
      "Time of epoch 3: 153.32353687286377 seconds\n",
      "Epoch 3 training loss: 684.9062884115853\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 156.1304225921631 seconds\n",
      "Epoch 1 training loss: 677.3883804924972\n",
      "Time of epoch 2: 162.87188625335693 seconds\n",
      "Epoch 2 training loss: 672.8294959878649\n",
      "Time of epoch 3: 160.33416724205017 seconds\n",
      "Epoch 3 training loss: 672.3217467062307\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 162.44576478004456 seconds\n",
      "Epoch 1 training loss: 676.5388087260077\n",
      "Time of epoch 2: 163.0329179763794 seconds\n",
      "Epoch 2 training loss: 667.4572878052193\n",
      "Time of epoch 3: 160.26748895645142 seconds\n",
      "Epoch 3 training loss: 663.3469901311285\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 160.9442687034607 seconds\n",
      "Epoch 1 training loss: 685.4753703540368\n",
      "Time of epoch 2: 160.01363110542297 seconds\n",
      "Epoch 2 training loss: 678.389061232859\n",
      "Time of epoch 1: 161.36807990074158 seconds\n",
      "Epoch 1 training loss: 709.6463893678535\n",
      "Time of epoch 2: 155.79393577575684 seconds\n",
      "Epoch 2 training loss: 687.0910725804456\n",
      "Time of epoch 3: 157.06555819511414 seconds\n",
      "Epoch 3 training loss: 685.2081662184853\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 160.34332013130188 seconds\n",
      "Epoch 1 training loss: 41200.58814890357\n",
      "Time of epoch 2: 157.76385855674744 seconds\n",
      "Epoch 2 training loss: 684.9116064685136\n",
      "Time of epoch 3: 152.5993366241455 seconds\n",
      "Epoch 3 training loss: 684.9118000185878\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 143.11906957626343 seconds\n",
      "Epoch 1 training loss: 677.6992836242645\n",
      "Time of epoch 2: 163.93209385871887 seconds\n",
      "Epoch 2 training loss: 673.2637899226522\n",
      "Time of epoch 3: 138.39920616149902 seconds\n",
      "Epoch 3 training loss: 672.682538328384\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 134.3738775253296 seconds\n",
      "Epoch 1 training loss: 675.8097657290045\n",
      "Time of epoch 2: 137.76169657707214 seconds\n",
      "Epoch 2 training loss: 665.8265656295646\n",
      "Time of epoch 3: 136.35151934623718 seconds\n",
      "Epoch 3 training loss: 661.2014703860433\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 137.73356437683105 seconds\n",
      "Epoch 1 training loss: 682.8135166869431\n",
      "Time of epoch 2: 144.71451902389526 seconds\n",
      "Epoch 2 training loss: 675.3480464373991\n",
      "Time of epoch 3: 141.70125341415405 seconds\n",
      "Epoch 3 training loss: 671.3401483786782\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 140.76039838790894 seconds\n",
      "Epoch 1 training loss: 705.0441629653237\n",
      "Time of epoch 2: 144.54542183876038 seconds\n",
      "Epoch 2 training loss: 685.4368613479401\n",
      "Time of epoch 3: 142.05289387702942 seconds\n",
      "Epoch 3 training loss: 683.6747215656363\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 193.52236557006836 seconds\n",
      "Epoch 1 training loss: 9907.867188099392\n",
      "Time of epoch 2: 195.65834641456604 seconds\n",
      "Epoch 2 training loss: 684.9039572127374\n",
      "Time of epoch 3: 198.12304949760437 seconds\n",
      "Epoch 3 training loss: 684.9071230096274\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 192.2273383140564 seconds\n",
      "Epoch 1 training loss: 679.4525431226956\n",
      "Time of epoch 2: 194.2788097858429 seconds\n",
      "Epoch 2 training loss: 682.0531936374039\n",
      "Time of epoch 3: 189.76127219200134 seconds\n",
      "Epoch 3 training loss: 688.5900444857288\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 189.42112636566162 seconds\n",
      "Epoch 1 training loss: 677.6745187318272\n",
      "Time of epoch 2: 193.19715476036072 seconds\n",
      "Epoch 2 training loss: 667.0144485196392\n",
      "Time of epoch 3: 204.05703353881836 seconds\n",
      "Epoch 3 training loss: 662.4059188051601\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 206.19769072532654 seconds\n",
      "Epoch 1 training loss: 682.8474184171157\n",
      "Time of epoch 2: 206.567773103714 seconds\n",
      "Epoch 2 training loss: 675.4571295901666\n",
      "Time of epoch 3: 207.2100372314453 seconds\n",
      "Epoch 3 training loss: 670.6938023772614\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 256, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 206.22501230239868 seconds\n",
      "Epoch 1 training loss: 703.22189578994\n",
      "Time of epoch 2: 210.17693638801575 seconds\n",
      "Epoch 2 training loss: 685.9769369869548\n",
      "Time of epoch 3: 212.3668234348297 seconds\n",
      "Epoch 3 training loss: 684.1186409098024\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 133.0745930671692 seconds\n",
      "Epoch 1 training loss: 2932.307630991886\n",
      "Time of epoch 2: 132.84934401512146 seconds\n",
      "Epoch 2 training loss: 684.9080214828856\n",
      "Time of epoch 3: 127.29002809524536 seconds\n",
      "Epoch 3 training loss: 684.9069378116483\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 130.77438616752625 seconds\n",
      "Epoch 1 training loss: 674.9511355755734\n",
      "Time of epoch 2: 130.09607791900635 seconds\n",
      "Epoch 2 training loss: 669.075655047296\n",
      "Time of epoch 3: 123.86493372917175 seconds\n",
      "Epoch 3 training loss: 668.1533339678487\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 130.429682970047 seconds\n",
      "Epoch 1 training loss: 673.2493678185081\n",
      "Time of epoch 2: 154.320698261261 seconds\n",
      "Epoch 2 training loss: 663.6565409543485\n",
      "Time of epoch 1: 152.73621535301208 seconds\n",
      "Epoch 1 training loss: 682.8730744349211\n",
      "Time of epoch 2: 151.4011209011078 seconds\n",
      "Epoch 2 training loss: 675.2257631884547\n",
      "Time of epoch 3: 142.9497423171997 seconds\n",
      "Epoch 3 training loss: 671.388167332851\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 147.20104598999023 seconds\n",
      "Epoch 1 training loss: 703.1174598516765\n",
      "Time of epoch 2: 148.4964735507965 seconds\n",
      "Epoch 2 training loss: 684.89106689591\n",
      "Time of epoch 3: 148.9601969718933 seconds\n",
      "Epoch 3 training loss: 682.8838420478374\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 146.92299723625183 seconds\n",
      "Epoch 1 training loss: 10055.113984337582\n",
      "Time of epoch 2: 149.54872918128967 seconds\n",
      "Epoch 2 training loss: 684.913175420558\n",
      "Time of epoch 3: 146.51851844787598 seconds\n",
      "Epoch 3 training loss: 684.9136628524254\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 145.90238904953003 seconds\n",
      "Epoch 1 training loss: 675.7449835789018\n",
      "Time of epoch 2: 147.13222193717957 seconds\n",
      "Epoch 2 training loss: 670.2978584563817\n",
      "Time of epoch 3: 141.56163096427917 seconds\n",
      "Epoch 3 training loss: 669.8240411971516\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 145.23747420310974 seconds\n",
      "Epoch 1 training loss: 672.5987109270442\n",
      "Time of epoch 2: 143.7471661567688 seconds\n",
      "Epoch 2 training loss: 663.0419134048257\n",
      "Time of epoch 3: 144.01578330993652 seconds\n",
      "Epoch 3 training loss: 659.4421634002993\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 145.26329469680786 seconds\n",
      "Epoch 1 training loss: 681.8412751654024\n",
      "Time of epoch 2: 153.78201913833618 seconds\n",
      "Epoch 2 training loss: 674.3624945606363\n",
      "Time of epoch 3: 151.8864984512329 seconds\n",
      "Epoch 3 training loss: 670.2011634893476\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 155.86283016204834 seconds\n",
      "Epoch 1 training loss: 700.5563822091287\n",
      "Time of epoch 2: 157.79526472091675 seconds\n",
      "Epoch 2 training loss: 683.6634360056255\n",
      "Time of epoch 3: 152.07815885543823 seconds\n",
      "Epoch 3 training loss: 681.8773405096571\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 155.97656869888306 seconds\n",
      "Epoch 1 training loss: 10410.132736305055\n",
      "Time of epoch 2: 169.83414578437805 seconds\n",
      "Epoch 2 training loss: 684.9183870883304\n",
      "Time of epoch 3: 159.78576683998108 seconds\n",
      "Epoch 3 training loss: 684.9097136551635\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 168.07800269126892 seconds\n",
      "Epoch 1 training loss: 679.538012503588\n",
      "Time of epoch 2: 167.5652482509613 seconds\n",
      "Epoch 2 training loss: 686.123320398637\n",
      "Time of epoch 3: 163.87347149848938 seconds\n",
      "Epoch 3 training loss: 687.1330191468865\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 165.60341668128967 seconds\n",
      "Epoch 1 training loss: 676.0470716890536\n",
      "Time of epoch 2: 167.25683617591858 seconds\n",
      "Epoch 2 training loss: 662.6584066802783\n",
      "Time of epoch 3: 166.9399392604828 seconds\n",
      "Epoch 3 training loss: 658.315539427304\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 164.02372479438782 seconds\n",
      "Epoch 1 training loss: 680.6583244091763\n",
      "Time of epoch 2: 172.4715039730072 seconds\n",
      "Epoch 2 training loss: 673.9197192365464\n",
      "Time of epoch 3: 164.52189683914185 seconds\n",
      "Epoch 3 training loss: 669.5506920169258\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 162.43397116661072 seconds\n",
      "Epoch 1 training loss: 697.0512132033297\n",
      "Time of epoch 2: 166.7796905040741 seconds\n",
      "Epoch 2 training loss: 682.4617721763644\n",
      "Time of epoch 3: 164.8249909877777 seconds\n",
      "Epoch 3 training loss: 680.7431812448243\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 165.10731554031372 seconds\n",
      "Epoch 1 training loss: 26818.666174063113\n",
      "Time of epoch 2: 162.08064818382263 seconds\n",
      "Epoch 2 training loss: 684.9056192698189\n",
      "Time of epoch 3: 166.84957695007324 seconds\n",
      "Epoch 3 training loss: 684.9073325340875\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 160.52587389945984 seconds\n",
      "Epoch 1 training loss: 677.642616571623\n",
      "Time of epoch 2: 163.19079542160034 seconds\n",
      "Epoch 2 training loss: 678.7554309988562\n",
      "Time of epoch 3: 168.83567357063293 seconds\n",
      "Epoch 3 training loss: 686.3668549444224\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 160.32555770874023 seconds\n",
      "Epoch 1 training loss: 672.3899407517246\n",
      "Time of epoch 2: 136.21691155433655 seconds\n",
      "Epoch 2 training loss: 662.5877501794797\n",
      "Time of epoch 3: 134.2346794605255 seconds\n",
      "Epoch 3 training loss: 658.9069317571115\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 134.0922200679779 seconds\n",
      "Epoch 1 training loss: 679.3031637395209\n",
      "Time of epoch 2: 133.72819066047668 seconds\n",
      "Epoch 2 training loss: 671.8731914866911\n",
      "Time of epoch 3: 133.03003549575806 seconds\n",
      "Epoch 3 training loss: 665.821430576012\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 134.5778350830078 seconds\n",
      "Epoch 1 training loss: 694.1820176110961\n",
      "Time of epoch 2: 133.61651515960693 seconds\n",
      "Epoch 2 training loss: 681.1786700918951\n",
      "Time of epoch 3: 134.99883723258972 seconds\n",
      "Epoch 3 training loss: 679.4954616260338\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 193.02492141723633 seconds\n",
      "Epoch 1 training loss: 38623.19540144288\n",
      "Time of epoch 2: 192.86437249183655 seconds\n",
      "Epoch 2 training loss: 684.9046067114435\n",
      "Time of epoch 3: 193.40753364562988 seconds\n",
      "Epoch 3 training loss: 684.9099161415388\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 211.20886135101318 seconds\n",
      "Epoch 1 training loss: 678.5860232347528\n",
      "Time of epoch 2: 208.94071292877197 seconds\n",
      "Epoch 2 training loss: 675.9305116816637\n",
      "Time of epoch 3: 211.94652104377747 seconds\n",
      "Epoch 3 training loss: 677.1034181910627\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 210.73503923416138 seconds\n",
      "Epoch 1 training loss: 673.0165590279402\n",
      "Time of epoch 2: 210.464102268219 seconds\n",
      "Epoch 2 training loss: 663.1117360315836\n",
      "Time of epoch 3: 210.13499927520752 seconds\n",
      "Epoch 3 training loss: 659.0460626738102\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 221.34577703475952 seconds\n",
      "Epoch 1 training loss: 679.2140840520279\n",
      "Time of epoch 2: 212.64946722984314 seconds\n",
      "Epoch 2 training loss: 670.2260178289879\n",
      "Time of epoch 3: 211.94572401046753 seconds\n",
      "Epoch 3 training loss: 664.6694016655305\n",
      "-------------------------------\n",
      "batch size: 32, hidden_size: 512, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 214.45112133026123 seconds\n",
      "Epoch 1 training loss: 694.1239084774174\n",
      "Time of epoch 2: 212.24497604370117 seconds\n",
      "Epoch 2 training loss: 681.7183870500114\n",
      "Time of epoch 3: 213.4859848022461 seconds\n",
      "Epoch 3 training loss: 679.8053207252523\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 80.79925084114075 seconds\n",
      "Epoch 1 training loss: 689.5922866263949\n",
      "Time of epoch 2: 81.07719039916992 seconds\n",
      "Epoch 2 training loss: 684.8996119056915\n",
      "Time of epoch 3: 81.72530698776245 seconds\n",
      "Epoch 3 training loss: 684.895981621935\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 86.38536715507507 seconds\n",
      "Epoch 1 training loss: 689.9020669875722\n",
      "Time of epoch 2: 78.40269947052002 seconds\n",
      "Epoch 2 training loss: 677.8631143285178\n",
      "Time of epoch 3: 76.51443219184875 seconds\n",
      "Epoch 3 training loss: 671.2347619817264\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 77.26193928718567 seconds\n",
      "Epoch 1 training loss: 691.1752941978626\n",
      "Time of epoch 2: 79.83137226104736 seconds\n",
      "Epoch 2 training loss: 683.6480451284107\n",
      "Time of epoch 3: 78.36363577842712 seconds\n",
      "Epoch 3 training loss: 678.9753411626319\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 79.62936115264893 seconds\n",
      "Epoch 1 training loss: 708.8779544657511\n",
      "Time of epoch 2: 79.07673501968384 seconds\n",
      "Epoch 2 training loss: 693.3656302450212\n",
      "Time of epoch 3: 79.52092027664185 seconds\n",
      "Epoch 3 training loss: 691.5783388506183\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 79.9822747707367 seconds\n",
      "Epoch 1 training loss: 886.9457710265002\n",
      "Time of epoch 2: 80.9016170501709 seconds\n",
      "Epoch 2 training loss: 721.390731163441\n",
      "Time of epoch 3: 82.16048860549927 seconds\n",
      "Epoch 3 training loss: 719.2120998569507\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 78.6015694141388 seconds\n",
      "Epoch 1 training loss: 695.046481435808\n",
      "Time of epoch 2: 81.59175229072571 seconds\n",
      "Epoch 2 training loss: 684.8946075700613\n",
      "Time of epoch 3: 80.39202499389648 seconds\n",
      "Epoch 3 training loss: 685.0001032246007\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 80.65750408172607 seconds\n",
      "Epoch 1 training loss: 701.8488155685159\n",
      "Time of epoch 2: 85.31086850166321 seconds\n",
      "Epoch 2 training loss: 683.586151905618\n",
      "Time of epoch 3: 79.62162661552429 seconds\n",
      "Epoch 3 training loss: 672.7035921105721\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 82.69802474975586 seconds\n",
      "Epoch 1 training loss: 722.1476474214637\n",
      "Time of epoch 2: 81.87625741958618 seconds\n",
      "Epoch 2 training loss: 713.3678423967644\n",
      "Time of epoch 3: 79.83689880371094 seconds\n",
      "Epoch 3 training loss: 707.0075569111123\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 79.51348328590393 seconds\n",
      "Epoch 1 training loss: 756.6732970454524\n",
      "Time of epoch 2: 75.48682475090027 seconds\n",
      "Epoch 2 training loss: 741.3466768820954\n",
      "Time of epoch 3: 78.50345993041992 seconds\n",
      "Epoch 3 training loss: 739.9390693960995\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 82.18048691749573 seconds\n",
      "Epoch 1 training loss: 822.7469612671317\n",
      "Time of epoch 2: 75.69936108589172 seconds\n",
      "Epoch 2 training loss: 701.8571081175847\n",
      "Time of epoch 3: 78.43623971939087 seconds\n",
      "Epoch 3 training loss: 699.6158423188873\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 76.04836940765381 seconds\n",
      "Epoch 1 training loss: 697.3208697529229\n",
      "Time of epoch 2: 75.54271459579468 seconds\n",
      "Epoch 2 training loss: 684.9161365194343\n",
      "Time of epoch 3: 80.03796148300171 seconds\n",
      "Epoch 3 training loss: 684.8979475171695\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 78.10955309867859 seconds\n",
      "Epoch 1 training loss: 686.8185802009591\n",
      "Time of epoch 2: 82.97919797897339 seconds\n",
      "Epoch 2 training loss: 675.5544557168159\n",
      "Time of epoch 3: 79.6179871559143 seconds\n",
      "Epoch 3 training loss: 669.6160938532305\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 80.12418699264526 seconds\n",
      "Epoch 1 training loss: 700.0959193562115\n",
      "Time of epoch 2: 81.50706911087036 seconds\n",
      "Epoch 2 training loss: 690.5193390518439\n",
      "Time of epoch 3: 80.00247859954834 seconds\n",
      "Epoch 3 training loss: 682.0221341996612\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 75.93079328536987 seconds\n",
      "Epoch 1 training loss: 714.4127619280533\n",
      "Time of epoch 2: 77.56427264213562 seconds\n",
      "Epoch 2 training loss: 700.7905740507222\n",
      "Time of epoch 3: 80.2601089477539 seconds\n",
      "Epoch 3 training loss: 698.9607546609558\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 76.43881106376648 seconds\n",
      "Epoch 1 training loss: 848.2837195073597\n",
      "Time of epoch 2: 78.46185302734375 seconds\n",
      "Epoch 2 training loss: 721.3309553054897\n",
      "Time of epoch 3: 78.89683890342712 seconds\n",
      "Epoch 3 training loss: 719.1217571286622\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 77.7446026802063 seconds\n",
      "Epoch 1 training loss: 700.8099388262414\n",
      "Time of epoch 2: 78.36714959144592 seconds\n",
      "Epoch 2 training loss: 684.8964415213395\n",
      "Time of epoch 3: 78.20756649971008 seconds\n",
      "Epoch 3 training loss: 684.9298004268666\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 80.00777316093445 seconds\n",
      "Epoch 1 training loss: 686.9829313151968\n",
      "Time of epoch 2: 79.36092972755432 seconds\n",
      "Epoch 2 training loss: 675.5448981065975\n",
      "Time of epoch 3: 82.31735825538635 seconds\n",
      "Epoch 3 training loss: 670.5954764295088\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 80.75106024742126 seconds\n",
      "Epoch 1 training loss: 692.364174053706\n",
      "Time of epoch 2: 80.25065898895264 seconds\n",
      "Epoch 2 training loss: 684.3346157690266\n",
      "Time of epoch 3: 79.90299916267395 seconds\n",
      "Epoch 3 training loss: 678.4019193740709\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 77.57736492156982 seconds\n",
      "Epoch 1 training loss: 713.0340565597616\n",
      "Time of epoch 2: 76.46318054199219 seconds\n",
      "Epoch 2 training loss: 700.1911008667757\n",
      "Time of epoch 3: 76.21775126457214 seconds\n",
      "Epoch 3 training loss: 696.6039403760287\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 77.14536213874817 seconds\n",
      "Epoch 1 training loss: 779.2194702265562\n",
      "Time of epoch 2: 81.69601821899414 seconds\n",
      "Epoch 2 training loss: 700.3871159166398\n",
      "Time of epoch 3: 75.58130025863647 seconds\n",
      "Epoch 3 training loss: 698.1104028470173\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 101.2929961681366 seconds\n",
      "Epoch 1 training loss: 694.8655592320986\n",
      "Time of epoch 2: 100.26758980751038 seconds\n",
      "Epoch 2 training loss: 684.9041404051324\n",
      "Time of epoch 3: 101.54208874702454 seconds\n",
      "Epoch 3 training loss: 684.897133150094\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 100.37484455108643 seconds\n",
      "Epoch 1 training loss: 689.527867365005\n",
      "Time of epoch 2: 100.23286557197571 seconds\n",
      "Epoch 2 training loss: 678.9877662178071\n",
      "Time of epoch 3: 102.22776055335999 seconds\n",
      "Epoch 3 training loss: 672.5161323520267\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 100.39215993881226 seconds\n",
      "Epoch 1 training loss: 690.7158963672853\n",
      "Time of epoch 2: 99.78548812866211 seconds\n",
      "Epoch 2 training loss: 680.7671660302777\n",
      "Time of epoch 3: 100.71539902687073 seconds\n",
      "Epoch 3 training loss: 674.262375526445\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 101.82106590270996 seconds\n",
      "Epoch 1 training loss: 711.2472909890877\n",
      "Time of epoch 2: 98.7738823890686 seconds\n",
      "Epoch 2 training loss: 700.0431420413962\n",
      "Time of epoch 3: 102.22912096977234 seconds\n",
      "Epoch 3 training loss: 697.8798381636133\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 64, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 98.5290424823761 seconds\n",
      "Epoch 1 training loss: 788.4722553610228\n",
      "Time of epoch 2: 99.43157625198364 seconds\n",
      "Epoch 2 training loss: 703.1676033330428\n",
      "Time of epoch 3: 99.0640823841095 seconds\n",
      "Epoch 3 training loss: 700.3935515652435\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 74.87709903717041 seconds\n",
      "Epoch 1 training loss: 704.367214337878\n",
      "Time of epoch 2: 79.96823024749756 seconds\n",
      "Epoch 2 training loss: 684.8970218024612\n",
      "Time of epoch 3: 78.74818181991577 seconds\n",
      "Epoch 3 training loss: 684.898862492276\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 89.42365145683289 seconds\n",
      "Epoch 1 training loss: 681.3636362000051\n",
      "Time of epoch 2: 84.83563876152039 seconds\n",
      "Epoch 2 training loss: 670.6515990179811\n",
      "Time of epoch 3: 82.37877178192139 seconds\n",
      "Epoch 3 training loss: 666.2533522544721\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 77.84262180328369 seconds\n",
      "Epoch 1 training loss: 687.0393449998869\n",
      "Time of epoch 2: 80.76962804794312 seconds\n",
      "Epoch 2 training loss: 677.4782849378312\n",
      "Time of epoch 3: 76.72195029258728 seconds\n",
      "Epoch 3 training loss: 673.0594428643928\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 76.55370140075684 seconds\n",
      "Epoch 1 training loss: 697.4316120780398\n",
      "Time of epoch 2: 78.53994464874268 seconds\n",
      "Epoch 2 training loss: 685.06025339224\n",
      "Time of epoch 3: 75.1429181098938 seconds\n",
      "Epoch 3 training loss: 683.045212394601\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 76.74875211715698 seconds\n",
      "Epoch 1 training loss: 790.9622842591359\n",
      "Time of epoch 2: 77.37273263931274 seconds\n",
      "Epoch 2 training loss: 696.5979845289671\n",
      "Time of epoch 3: 76.2054660320282 seconds\n",
      "Epoch 3 training loss: 694.0502006387094\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 77.08548617362976 seconds\n",
      "Epoch 1 training loss: 710.4283219726659\n",
      "Time of epoch 2: 81.88652873039246 seconds\n",
      "Epoch 2 training loss: 684.910367719111\n",
      "Time of epoch 3: 89.0396203994751 seconds\n",
      "Epoch 3 training loss: 684.894789183826\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 83.945476770401 seconds\n",
      "Epoch 1 training loss: 677.8837156699678\n",
      "Time of epoch 2: 78.45223951339722 seconds\n",
      "Epoch 2 training loss: 670.1683579255579\n",
      "Time of epoch 3: 77.31438398361206 seconds\n",
      "Epoch 3 training loss: 667.0950277491764\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 77.08965182304382 seconds\n",
      "Epoch 1 training loss: 685.8033824996526\n",
      "Time of epoch 2: 77.60239362716675 seconds\n",
      "Epoch 2 training loss: 676.0963842265588\n",
      "Time of epoch 3: 75.4043025970459 seconds\n",
      "Epoch 3 training loss: 671.2141414222511\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 75.03322291374207 seconds\n",
      "Epoch 1 training loss: 697.241619002027\n",
      "Time of epoch 2: 75.7217755317688 seconds\n",
      "Epoch 2 training loss: 685.6290881888287\n",
      "Time of epoch 3: 77.70646858215332 seconds\n",
      "Epoch 3 training loss: 683.7155416721658\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 78.84733438491821 seconds\n",
      "Epoch 1 training loss: 765.2275939373837\n",
      "Time of epoch 2: 77.65973281860352 seconds\n",
      "Epoch 2 training loss: 694.3464457621906\n",
      "Time of epoch 3: 78.03079962730408 seconds\n",
      "Epoch 3 training loss: 692.5074245840223\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 76.98517966270447 seconds\n",
      "Epoch 1 training loss: 4179.490812323583\n",
      "Time of epoch 2: 78.61997389793396 seconds\n",
      "Epoch 2 training loss: 684.9000034784191\n",
      "Time of epoch 3: 78.27155470848083 seconds\n",
      "Epoch 3 training loss: 684.9104041943243\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 77.07247471809387 seconds\n",
      "Epoch 1 training loss: 680.5302694136261\n",
      "Time of epoch 2: 76.79163074493408 seconds\n",
      "Epoch 2 training loss: 670.78538077034\n",
      "Time of epoch 3: 79.65039157867432 seconds\n",
      "Epoch 3 training loss: 668.1815452854675\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 78.18706798553467 seconds\n",
      "Epoch 1 training loss: 684.1804045124458\n",
      "Time of epoch 2: 77.63168740272522 seconds\n",
      "Epoch 2 training loss: 673.7845687131094\n",
      "Time of epoch 3: 80.78173208236694 seconds\n",
      "Epoch 3 training loss: 669.781104146449\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 81.54472255706787 seconds\n",
      "Epoch 1 training loss: 700.2133290409462\n",
      "Time of epoch 2: 75.30501198768616 seconds\n",
      "Epoch 2 training loss: 689.0877347467394\n",
      "Time of epoch 3: 84.76655554771423 seconds\n",
      "Epoch 3 training loss: 686.7752915697451\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 88.03300285339355 seconds\n",
      "Epoch 1 training loss: 763.4248369883749\n",
      "Time of epoch 2: 77.67265272140503 seconds\n",
      "Epoch 2 training loss: 695.392397518549\n",
      "Time of epoch 3: 77.40330123901367 seconds\n",
      "Epoch 3 training loss: 693.2438539805404\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 77.76529383659363 seconds\n",
      "Epoch 1 training loss: 755.9315413623817\n",
      "Time of epoch 2: 82.92770028114319 seconds\n",
      "Epoch 2 training loss: 684.8994422854546\n",
      "Time of epoch 3: 83.98360776901245 seconds\n",
      "Epoch 3 training loss: 684.8973556864622\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 82.59558439254761 seconds\n",
      "Epoch 1 training loss: 680.2582379160734\n",
      "Time of epoch 2: 75.08144783973694 seconds\n",
      "Epoch 2 training loss: 672.3288308733607\n",
      "Time of epoch 3: 76.85828018188477 seconds\n",
      "Epoch 3 training loss: 669.9564968143495\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 78.02096343040466 seconds\n",
      "Epoch 1 training loss: 684.458633973344\n",
      "Time of epoch 2: 77.01183915138245 seconds\n",
      "Epoch 2 training loss: 674.638868062022\n",
      "Time of epoch 3: 77.2115204334259 seconds\n",
      "Epoch 3 training loss: 671.5320728315642\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 80.67069983482361 seconds\n",
      "Epoch 1 training loss: 695.1972668123625\n",
      "Time of epoch 2: 75.39261293411255 seconds\n",
      "Epoch 2 training loss: 684.8830103940826\n",
      "Time of epoch 3: 83.2008843421936 seconds\n",
      "Epoch 3 training loss: 682.7552475273598\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 81.64465045928955 seconds\n",
      "Epoch 1 training loss: 748.4105848251096\n",
      "Time of epoch 2: 75.42936849594116 seconds\n",
      "Epoch 2 training loss: 695.1280275167541\n",
      "Time of epoch 3: 78.03737545013428 seconds\n",
      "Epoch 3 training loss: 693.0890533532025\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 101.68792486190796 seconds\n",
      "Epoch 1 training loss: 1510.7693428255207\n",
      "Time of epoch 2: 104.18491387367249 seconds\n",
      "Epoch 2 training loss: 684.8962635097229\n",
      "Time of epoch 3: 107.25184941291809 seconds\n",
      "Epoch 3 training loss: 684.9350079667853\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 106.52491092681885 seconds\n",
      "Epoch 1 training loss: 683.3003268716726\n",
      "Time of epoch 2: 105.89911460876465 seconds\n",
      "Epoch 2 training loss: 674.0050624049255\n",
      "Time of epoch 3: 103.61445212364197 seconds\n",
      "Epoch 3 training loss: 670.0042135255627\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 106.26425170898438 seconds\n",
      "Epoch 1 training loss: 685.8378090348274\n",
      "Time of epoch 2: 101.18388772010803 seconds\n",
      "Epoch 2 training loss: 675.0994159088714\n",
      "Time of epoch 3: 104.1736273765564 seconds\n",
      "Epoch 3 training loss: 667.713590407892\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 106.24905037879944 seconds\n",
      "Epoch 1 training loss: 693.935113402743\n",
      "Time of epoch 2: 103.72796320915222 seconds\n",
      "Epoch 2 training loss: 684.3861415448608\n",
      "Time of epoch 3: 99.38307547569275 seconds\n",
      "Epoch 3 training loss: 681.8190711283945\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 128, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 104.21379542350769 seconds\n",
      "Epoch 1 training loss: 745.5565994634248\n",
      "Time of epoch 2: 107.77052617073059 seconds\n",
      "Epoch 2 training loss: 696.4171904233417\n",
      "Time of epoch 3: 103.54979014396667 seconds\n",
      "Epoch 3 training loss: 694.0157230485709\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 79.71111130714417 seconds\n",
      "Epoch 1 training loss: 931.3011119082189\n",
      "Time of epoch 2: 75.42489409446716 seconds\n",
      "Epoch 2 training loss: 684.8960733512951\n",
      "Time of epoch 3: 77.00664710998535 seconds\n",
      "Epoch 3 training loss: 684.8954579310843\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 82.9909279346466 seconds\n",
      "Epoch 1 training loss: 675.6983959509686\n",
      "Time of epoch 2: 87.73765158653259 seconds\n",
      "Epoch 2 training loss: 667.9048633069679\n",
      "Time of epoch 1: 86.37380909919739 seconds\n",
      "Epoch 1 training loss: 679.5726672884429\n",
      "Time of epoch 2: 85.71972990036011 seconds\n",
      "Epoch 2 training loss: 669.3923565405692\n",
      "Time of epoch 3: 85.84347796440125 seconds\n",
      "Epoch 3 training loss: 666.5268517393903\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 85.0144612789154 seconds\n",
      "Epoch 1 training loss: 691.3492637310568\n",
      "Time of epoch 2: 87.2353847026825 seconds\n",
      "Epoch 2 training loss: 681.5424839962967\n",
      "Time of epoch 3: 84.98240733146667 seconds\n",
      "Epoch 3 training loss: 679.2992561094113\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 83.69645762443542 seconds\n",
      "Epoch 1 training loss: 744.5544693180304\n",
      "Time of epoch 2: 82.31314373016357 seconds\n",
      "Epoch 2 training loss: 691.3789946904972\n",
      "Time of epoch 3: 80.80297660827637 seconds\n",
      "Epoch 3 training loss: 689.0908331642665\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 82.75059247016907 seconds\n",
      "Epoch 1 training loss: 1432.5170303564573\n",
      "Time of epoch 2: 81.5912516117096 seconds\n",
      "Epoch 2 training loss: 684.8976578515166\n",
      "Time of epoch 3: 79.30950045585632 seconds\n",
      "Epoch 3 training loss: 684.896524252768\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 85.09953832626343 seconds\n",
      "Epoch 1 training loss: 677.0626053011061\n",
      "Time of epoch 2: 89.87657451629639 seconds\n",
      "Epoch 2 training loss: 668.9095138316729\n",
      "Time of epoch 3: 91.48299741744995 seconds\n",
      "Epoch 3 training loss: 666.6602292918319\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 86.40837526321411 seconds\n",
      "Epoch 1 training loss: 678.1593953388572\n",
      "Time of epoch 2: 76.10074639320374 seconds\n",
      "Epoch 2 training loss: 667.6256220775747\n",
      "Time of epoch 3: 82.34915137290955 seconds\n",
      "Epoch 3 training loss: 664.2141515471246\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 90.99242734909058 seconds\n",
      "Epoch 1 training loss: 690.1767051578585\n",
      "Time of epoch 2: 91.45984554290771 seconds\n",
      "Epoch 2 training loss: 680.551667324212\n",
      "Time of epoch 3: 88.50198125839233 seconds\n",
      "Epoch 3 training loss: 678.4953796402743\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 85.47821068763733 seconds\n",
      "Epoch 1 training loss: 734.3912656102683\n",
      "Time of epoch 2: 90.0308575630188 seconds\n",
      "Epoch 2 training loss: 689.1838324245807\n",
      "Time of epoch 3: 91.96833038330078 seconds\n",
      "Epoch 3 training loss: 687.4840686760168\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 91.45883536338806 seconds\n",
      "Epoch 1 training loss: 2199.051321624758\n",
      "Time of epoch 2: 89.20028400421143 seconds\n",
      "Epoch 2 training loss: 684.9045563207759\n",
      "Time of epoch 3: 85.86934232711792 seconds\n",
      "Epoch 3 training loss: 684.8996779222506\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 88.93977952003479 seconds\n",
      "Epoch 1 training loss: 676.2527721861633\n",
      "Time of epoch 2: 87.58204126358032 seconds\n",
      "Epoch 2 training loss: 668.9077390918201\n",
      "Time of epoch 3: 87.89557909965515 seconds\n",
      "Epoch 3 training loss: 667.316473428129\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 84.52602243423462 seconds\n",
      "Epoch 1 training loss: 676.6695177325325\n",
      "Time of epoch 2: 86.14572620391846 seconds\n",
      "Epoch 2 training loss: 666.8215066814104\n",
      "Time of epoch 3: 95.66220879554749 seconds\n",
      "Epoch 3 training loss: 662.8803157501699\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 91.57463049888611 seconds\n",
      "Epoch 1 training loss: 688.2185251382293\n",
      "Time of epoch 2: 87.41849327087402 seconds\n",
      "Epoch 2 training loss: 679.3465060455264\n",
      "Time of epoch 3: 82.70817947387695 seconds\n",
      "Epoch 3 training loss: 676.9521060391827\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 88.84091424942017 seconds\n",
      "Epoch 1 training loss: 729.5433751299066\n",
      "Time of epoch 2: 91.31292247772217 seconds\n",
      "Epoch 2 training loss: 688.7662942291307\n",
      "Time of epoch 3: 89.06261277198792 seconds\n",
      "Epoch 3 training loss: 686.5031919676451\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 84.48622679710388 seconds\n",
      "Epoch 1 training loss: 7020.5390590750085\n",
      "Time of epoch 2: 90.70621705055237 seconds\n",
      "Epoch 2 training loss: 684.8918604211851\n",
      "Time of epoch 3: 91.23016834259033 seconds\n",
      "Epoch 3 training loss: 684.8933392195069\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 92.11460185050964 seconds\n",
      "Epoch 1 training loss: 675.6025334753261\n",
      "Time of epoch 2: 93.40281391143799 seconds\n",
      "Epoch 2 training loss: 669.2637331650475\n",
      "Time of epoch 3: 68.1629409790039 seconds\n",
      "Epoch 3 training loss: 669.1459151591747\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 69.18645000457764 seconds\n",
      "Epoch 1 training loss: 675.579625227607\n",
      "Time of epoch 2: 69.70030188560486 seconds\n",
      "Epoch 2 training loss: 665.5152293644775\n",
      "Time of epoch 3: 68.24604797363281 seconds\n",
      "Epoch 3 training loss: 661.8657961712028\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 70.82682824134827 seconds\n",
      "Epoch 1 training loss: 684.8713322681331\n",
      "Time of epoch 2: 71.00250315666199 seconds\n",
      "Epoch 2 training loss: 676.110104312991\n",
      "Time of epoch 3: 71.17745542526245 seconds\n",
      "Epoch 3 training loss: 673.0383018201535\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 68.8985321521759 seconds\n",
      "Epoch 1 training loss: 720.3165873398735\n",
      "Time of epoch 2: 66.27986717224121 seconds\n",
      "Epoch 2 training loss: 686.9079779659841\n",
      "Time of epoch 3: 68.65159296989441 seconds\n",
      "Epoch 3 training loss: 684.7682065952744\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 95.55632781982422 seconds\n",
      "Epoch 1 training loss: 5687.298894502909\n",
      "Time of epoch 2: 96.34045767784119 seconds\n",
      "Epoch 2 training loss: 684.904038171748\n",
      "Time of epoch 3: 96.33749532699585 seconds\n",
      "Epoch 3 training loss: 684.8981543852151\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 95.06770300865173 seconds\n",
      "Epoch 1 training loss: 677.4315923617906\n",
      "Time of epoch 2: 95.10648608207703 seconds\n",
      "Epoch 2 training loss: 671.0167189312758\n",
      "Time of epoch 3: 95.59442186355591 seconds\n",
      "Epoch 3 training loss: 672.0113707854117\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 95.9078619480133 seconds\n",
      "Epoch 1 training loss: 677.0271422810308\n",
      "Time of epoch 2: 95.2893078327179 seconds\n",
      "Epoch 2 training loss: 667.0242533813207\n",
      "Time of epoch 3: 96.0783531665802 seconds\n",
      "Epoch 3 training loss: 660.8146145035239\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 95.65210199356079 seconds\n",
      "Epoch 1 training loss: 685.6510612366795\n",
      "Time of epoch 2: 102.32239937782288 seconds\n",
      "Epoch 2 training loss: 677.2250868277894\n",
      "Time of epoch 3: 105.44381761550903 seconds\n",
      "Epoch 3 training loss: 674.1338550898139\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 256, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 104.10994005203247 seconds\n",
      "Epoch 1 training loss: 714.1496211935494\n",
      "Time of epoch 2: 105.32807922363281 seconds\n",
      "Epoch 2 training loss: 686.1724625312343\n",
      "Time of epoch 3: 106.95045185089111 seconds\n",
      "Epoch 3 training loss: 684.2965179742672\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 92.43882417678833 seconds\n",
      "Epoch 1 training loss: 8008.7194878492355\n",
      "Time of epoch 2: 85.34896302223206 seconds\n",
      "Epoch 2 training loss: 684.8970314428091\n",
      "Time of epoch 3: 85.82945609092712 seconds\n",
      "Epoch 3 training loss: 684.9117156547054\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 64, lr: 0.01\n",
      "Time of epoch 2: 86.54946279525757 seconds\n",
      "Epoch 2 training loss: 665.9863570188961\n",
      "Time of epoch 3: 94.74479913711548 seconds\n",
      "Epoch 3 training loss: 664.6186237032936\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 94.99335384368896 seconds\n",
      "Epoch 1 training loss: 674.2533262025488\n",
      "Time of epoch 2: 90.79887509346008 seconds\n",
      "Epoch 2 training loss: 664.4245207629293\n",
      "Time of epoch 3: 91.50998544692993 seconds\n",
      "Epoch 3 training loss: 660.7916396175818\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 90.05183267593384 seconds\n",
      "Epoch 1 training loss: 686.3839302673041\n",
      "Time of epoch 2: 84.9752447605133 seconds\n",
      "Epoch 2 training loss: 678.0126403184616\n",
      "Time of epoch 3: 84.10682463645935 seconds\n",
      "Epoch 3 training loss: 675.208740085423\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 95.84599423408508 seconds\n",
      "Epoch 1 training loss: 717.7815988426362\n",
      "Time of epoch 2: 92.53715562820435 seconds\n",
      "Epoch 2 training loss: 685.8499190109179\n",
      "Time of epoch 3: 87.82156586647034 seconds\n",
      "Epoch 3 training loss: 683.9126202076786\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 83.99796676635742 seconds\n",
      "Epoch 1 training loss: 13545.585784444635\n",
      "Time of epoch 2: 89.21872520446777 seconds\n",
      "Epoch 2 training loss: 684.8990041293423\n",
      "Time of epoch 3: 91.48575639724731 seconds\n",
      "Epoch 3 training loss: 684.895697583838\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 89.9384412765503 seconds\n",
      "Epoch 1 training loss: 673.2372952058569\n",
      "Time of epoch 2: 90.51605701446533 seconds\n",
      "Epoch 2 training loss: 667.0308905771392\n",
      "Time of epoch 3: 93.39745593070984 seconds\n",
      "Epoch 3 training loss: 666.3537854042669\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 92.75416970252991 seconds\n",
      "Epoch 1 training loss: 674.0854471273095\n",
      "Time of epoch 2: 91.95406985282898 seconds\n",
      "Epoch 2 training loss: 664.2839107356677\n",
      "Time of epoch 3: 91.79764580726624 seconds\n",
      "Epoch 3 training loss: 659.9358166962048\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 94.32902359962463 seconds\n",
      "Epoch 1 training loss: 684.2925437677633\n",
      "Time of epoch 2: 92.27219295501709 seconds\n",
      "Epoch 2 training loss: 675.9302777425087\n",
      "Time of epoch 3: 93.10627722740173 seconds\n",
      "Epoch 3 training loss: 673.362283718405\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 92.08478212356567 seconds\n",
      "Epoch 1 training loss: 711.8705644706791\n",
      "Time of epoch 2: 92.84795379638672 seconds\n",
      "Epoch 2 training loss: 684.6521931960783\n",
      "Time of epoch 3: 92.75975561141968 seconds\n",
      "Epoch 3 training loss: 682.6533539470937\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 92.14712858200073 seconds\n",
      "Epoch 1 training loss: 19898.335059971145\n",
      "Time of epoch 2: 94.50244426727295 seconds\n",
      "Epoch 2 training loss: 684.9184820490335\n",
      "Time of epoch 3: 90.46897530555725 seconds\n",
      "Epoch 3 training loss: 684.8954626482581\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 91.85531616210938 seconds\n",
      "Epoch 1 training loss: 674.3017359719452\n",
      "Time of epoch 2: 93.0709662437439 seconds\n",
      "Epoch 2 training loss: 669.3891119812832\n",
      "Time of epoch 3: 93.02299904823303 seconds\n",
      "Epoch 3 training loss: 667.5759859276197\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 90.10027480125427 seconds\n",
      "Epoch 1 training loss: 673.0401437833478\n",
      "Time of epoch 2: 92.4335298538208 seconds\n",
      "Epoch 2 training loss: 663.3128376986912\n",
      "Time of epoch 3: 89.75310802459717 seconds\n",
      "Epoch 3 training loss: 659.0569131920843\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 91.0038640499115 seconds\n",
      "Epoch 1 training loss: 682.5873950363452\n",
      "Time of epoch 2: 90.926926612854 seconds\n",
      "Epoch 2 training loss: 674.8451117422419\n",
      "Time of epoch 3: 93.63419222831726 seconds\n",
      "Epoch 3 training loss: 671.7891659140492\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 93.82477355003357 seconds\n",
      "Epoch 1 training loss: 709.367777723387\n",
      "Time of epoch 2: 93.52641987800598 seconds\n",
      "Epoch 2 training loss: 684.2869412128598\n",
      "Time of epoch 3: 91.774174451828 seconds\n",
      "Epoch 3 training loss: 682.7274384011669\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 93.77320075035095 seconds\n",
      "Epoch 1 training loss: 61228.228733521246\n",
      "Time of epoch 2: 92.40639925003052 seconds\n",
      "Epoch 2 training loss: 684.9096529374891\n",
      "Time of epoch 3: 92.16317081451416 seconds\n",
      "Epoch 3 training loss: 684.895966295577\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 88.31764698028564 seconds\n",
      "Epoch 1 training loss: 674.8592791614008\n",
      "Time of epoch 2: 91.94545888900757 seconds\n",
      "Epoch 2 training loss: 669.4204409969664\n",
      "Time of epoch 3: 94.84991669654846 seconds\n",
      "Epoch 3 training loss: 668.6818332301441\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 91.15953850746155 seconds\n",
      "Epoch 1 training loss: 673.6576136921798\n",
      "Time of epoch 2: 93.08143973350525 seconds\n",
      "Epoch 2 training loss: 663.3019914313709\n",
      "Time of epoch 3: 92.41172313690186 seconds\n",
      "Epoch 3 training loss: 658.7455319190447\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 95.36147737503052 seconds\n",
      "Epoch 1 training loss: 681.3945664159461\n",
      "Time of epoch 2: 93.53679728507996 seconds\n",
      "Epoch 2 training loss: 673.5950441810938\n",
      "Time of epoch 3: 94.32996559143066 seconds\n",
      "Epoch 3 training loss: 669.798088121467\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 96.55594682693481 seconds\n",
      "Epoch 1 training loss: 703.4804163742751\n",
      "Time of epoch 2: 91.00065588951111 seconds\n",
      "Epoch 2 training loss: 682.7148801432907\n",
      "Time of epoch 3: 91.64903950691223 seconds\n",
      "Epoch 3 training loss: 680.9988363662783\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 110.75402808189392 seconds\n",
      "Epoch 1 training loss: 67000.36018134104\n",
      "Time of epoch 2: 112.21529126167297 seconds\n",
      "Epoch 2 training loss: 684.8962614364638\n",
      "Time of epoch 3: 112.9976978302002 seconds\n",
      "Epoch 3 training loss: 684.8963062308319\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 110.6861937046051 seconds\n",
      "Epoch 1 training loss: 675.9351476989581\n",
      "Time of epoch 2: 111.50528860092163 seconds\n",
      "Epoch 2 training loss: 670.8527134382637\n",
      "Time of epoch 3: 112.03830361366272 seconds\n",
      "Epoch 3 training loss: 669.79686037177\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 113.42665576934814 seconds\n",
      "Epoch 1 training loss: 673.1928183097117\n",
      "Time of epoch 2: 112.6004524230957 seconds\n",
      "Epoch 2 training loss: 662.967189264179\n",
      "Time of epoch 3: 111.47651934623718 seconds\n",
      "Epoch 3 training loss: 658.7064288227851\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 110.65134310722351 seconds\n",
      "Epoch 1 training loss: 681.1746804311946\n",
      "Time of epoch 2: 110.99442100524902 seconds\n",
      "Epoch 2 training loss: 673.2165068303075\n",
      "Time of epoch 3: 111.05533027648926 seconds\n",
      "Epoch 3 training loss: 668.6264835387029\n",
      "-------------------------------\n",
      "batch size: 64, hidden_size: 512, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 111.01114273071289 seconds\n",
      "Epoch 1 training loss: 701.3832986204335\n",
      "Time of epoch 3: 111.87758231163025 seconds\n",
      "Epoch 3 training loss: 680.4297930276397\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 50.32546305656433 seconds\n",
      "Epoch 1 training loss: 694.3711571469601\n",
      "Time of epoch 2: 49.76235222816467 seconds\n",
      "Epoch 2 training loss: 684.881583158956\n",
      "Time of epoch 3: 53.07047772407532 seconds\n",
      "Epoch 3 training loss: 684.8841301372087\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 52.477484703063965 seconds\n",
      "Epoch 1 training loss: 698.728347064207\n",
      "Time of epoch 2: 50.03327798843384 seconds\n",
      "Epoch 2 training loss: 685.977235535397\n",
      "Time of epoch 3: 47.38158941268921 seconds\n",
      "Epoch 3 training loss: 678.4224779392938\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 50.62989592552185 seconds\n",
      "Epoch 1 training loss: 699.9386243470717\n",
      "Time of epoch 2: 49.79025483131409 seconds\n",
      "Epoch 2 training loss: 691.712700969779\n",
      "Time of epoch 3: 49.073570013046265 seconds\n",
      "Epoch 3 training loss: 687.4146311153421\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 48.896498918533325 seconds\n",
      "Epoch 1 training loss: 725.847210416897\n",
      "Time of epoch 2: 50.00501847267151 seconds\n",
      "Epoch 2 training loss: 699.6297612351689\n",
      "Time of epoch 3: 53.89344048500061 seconds\n",
      "Epoch 3 training loss: 697.8868502756774\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 51.329570055007935 seconds\n",
      "Epoch 1 training loss: 966.7618274076289\n",
      "Time of epoch 2: 50.259424448013306 seconds\n",
      "Epoch 2 training loss: 760.2308853901758\n",
      "Time of epoch 3: 50.06640672683716 seconds\n",
      "Epoch 3 training loss: 712.6770924546365\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 50.59766125679016 seconds\n",
      "Epoch 1 training loss: 704.1037111035898\n",
      "Time of epoch 2: 48.28329825401306 seconds\n",
      "Epoch 2 training loss: 684.883545295062\n",
      "Time of epoch 3: 50.00572896003723 seconds\n",
      "Epoch 3 training loss: 684.8935551096666\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 49.35588097572327 seconds\n",
      "Epoch 1 training loss: 689.7384582143848\n",
      "Time of epoch 2: 50.719555616378784 seconds\n",
      "Epoch 2 training loss: 678.5456004831681\n",
      "Time of epoch 3: 48.75656461715698 seconds\n",
      "Epoch 3 training loss: 673.4996403582193\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 53.408186197280884 seconds\n",
      "Epoch 1 training loss: 702.3414507540616\n",
      "Time of epoch 2: 54.13107204437256 seconds\n",
      "Epoch 2 training loss: 691.0083130473065\n",
      "Time of epoch 3: 52.595393657684326 seconds\n",
      "Epoch 3 training loss: 685.6810044806875\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 55.132309675216675 seconds\n",
      "Epoch 1 training loss: 738.464380354498\n",
      "Time of epoch 2: 52.90950298309326 seconds\n",
      "Epoch 2 training loss: 713.6585865497414\n",
      "Time of epoch 3: 55.221163511276245 seconds\n",
      "Epoch 3 training loss: 712.3437055615185\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 55.42547345161438 seconds\n",
      "Epoch 1 training loss: 991.2815369505811\n",
      "Time of epoch 2: 56.195178747177124 seconds\n",
      "Epoch 2 training loss: 991.2079326524157\n",
      "Time of epoch 3: 54.47206711769104 seconds\n",
      "Epoch 3 training loss: 991.1179072752466\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 53.78006625175476 seconds\n",
      "Epoch 1 training loss: 713.3752724286319\n",
      "Time of epoch 2: 53.31289792060852 seconds\n",
      "Epoch 2 training loss: 684.9769963816136\n",
      "Time of epoch 3: 53.187530755996704 seconds\n",
      "Epoch 3 training loss: 684.8811987935926\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 53.45297288894653 seconds\n",
      "Epoch 1 training loss: 699.3189853136377\n",
      "Time of epoch 2: 57.02432155609131 seconds\n",
      "Epoch 2 training loss: 685.6321535082799\n",
      "Time of epoch 3: 55.256229639053345 seconds\n",
      "Epoch 3 training loss: 678.3230537124456\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 54.44747567176819 seconds\n",
      "Epoch 1 training loss: 713.6236589044757\n",
      "Time of epoch 2: 55.78031039237976 seconds\n",
      "Epoch 2 training loss: 705.1075661726362\n",
      "Time of epoch 3: 53.950247049331665 seconds\n",
      "Epoch 3 training loss: 700.0466248729135\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 53.741068601608276 seconds\n",
      "Epoch 1 training loss: 714.6966000844455\n",
      "Time of epoch 2: 53.97591710090637 seconds\n",
      "Epoch 2 training loss: 693.4409070549963\n",
      "Time of epoch 3: 53.60571050643921 seconds\n",
      "Epoch 3 training loss: 691.5050582576599\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 53.54116153717041 seconds\n",
      "Epoch 1 training loss: 914.384434617703\n",
      "Time of epoch 3: 53.74055886268616 seconds\n",
      "Epoch 3 training loss: 709.646255662506\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 53.83902597427368 seconds\n",
      "Epoch 1 training loss: 699.1483887086447\n",
      "Time of epoch 2: 55.72437810897827 seconds\n",
      "Epoch 2 training loss: 684.8770012225152\n",
      "Time of epoch 3: 52.69423270225525 seconds\n",
      "Epoch 3 training loss: 684.907260453693\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 56.4201922416687 seconds\n",
      "Epoch 1 training loss: 687.465543956284\n",
      "Time of epoch 2: 55.7053496837616 seconds\n",
      "Epoch 2 training loss: 677.9792300681644\n",
      "Time of epoch 3: 56.149346113204956 seconds\n",
      "Epoch 3 training loss: 673.1557871448484\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 56.28104090690613 seconds\n",
      "Epoch 1 training loss: 705.1915822913766\n",
      "Time of epoch 2: 55.44556665420532 seconds\n",
      "Epoch 2 training loss: 693.4900584371605\n",
      "Time of epoch 3: 54.07957863807678 seconds\n",
      "Epoch 3 training loss: 686.2898378854155\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 51.112250328063965 seconds\n",
      "Epoch 1 training loss: 721.7155380463244\n",
      "Time of epoch 2: 43.53344702720642 seconds\n",
      "Epoch 2 training loss: 701.8139973193623\n",
      "Time of epoch 3: 46.85882496833801 seconds\n",
      "Epoch 3 training loss: 699.8373883553237\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 45.24870586395264 seconds\n",
      "Epoch 1 training loss: 901.4420084091324\n",
      "Time of epoch 2: 43.7625515460968 seconds\n",
      "Epoch 2 training loss: 714.6811010088619\n",
      "Time of epoch 3: 43.8750741481781 seconds\n",
      "Epoch 3 training loss: 709.235159833318\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 1024, lr: 0.5\n",
      "Time of epoch 3: 52.42057728767395 seconds\n",
      "Epoch 3 training loss: 684.8802224025058\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 52.8721489906311 seconds\n",
      "Epoch 1 training loss: 688.5818809849385\n",
      "Time of epoch 2: 50.62295484542847 seconds\n",
      "Epoch 2 training loss: 677.913104720683\n",
      "Time of epoch 3: 51.68773865699768 seconds\n",
      "Epoch 3 training loss: 673.9962238930545\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 54.00730633735657 seconds\n",
      "Epoch 1 training loss: 698.5004786259126\n",
      "Time of epoch 2: 53.23419499397278 seconds\n",
      "Epoch 2 training loss: 690.7331503169623\n",
      "Time of epoch 3: 53.182180643081665 seconds\n",
      "Epoch 3 training loss: 681.2644554860444\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 53.945282220840454 seconds\n",
      "Epoch 1 training loss: 710.7084707359344\n",
      "Time of epoch 2: 54.10348057746887 seconds\n",
      "Epoch 2 training loss: 694.8884237342378\n",
      "Time of epoch 3: 52.77946925163269 seconds\n",
      "Epoch 3 training loss: 692.8568385425767\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 64, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 52.739145040512085 seconds\n",
      "Epoch 1 training loss: 845.1944578172141\n",
      "Time of epoch 2: 53.775142669677734 seconds\n",
      "Epoch 2 training loss: 711.8757510813107\n",
      "Time of epoch 3: 53.658326148986816 seconds\n",
      "Epoch 3 training loss: 708.367632582045\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 48.24707555770874 seconds\n",
      "Epoch 1 training loss: 702.1582347175361\n",
      "Time of epoch 2: 48.59109950065613 seconds\n",
      "Epoch 2 training loss: 684.8934683224345\n",
      "Time of epoch 3: 48.656445026397705 seconds\n",
      "Epoch 3 training loss: 684.8762585781153\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 49.80030155181885 seconds\n",
      "Epoch 1 training loss: 682.4653116667224\n",
      "Time of epoch 2: 49.404438734054565 seconds\n",
      "Epoch 2 training loss: 673.0334511777914\n",
      "Time of epoch 3: 46.056814193725586 seconds\n",
      "Epoch 3 training loss: 667.4600202259487\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 50.69722533226013 seconds\n",
      "Epoch 1 training loss: 685.1514910672234\n",
      "Time of epoch 2: 50.33783555030823 seconds\n",
      "Epoch 2 training loss: 676.4998257244816\n",
      "Time of epoch 3: 48.955862283706665 seconds\n",
      "Epoch 3 training loss: 671.6360863314035\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 49.25070929527283 seconds\n",
      "Epoch 1 training loss: 709.9848876646313\n",
      "Time of epoch 2: 49.0240159034729 seconds\n",
      "Epoch 2 training loss: 690.7713053160211\n",
      "Time of epoch 3: 49.127788066864014 seconds\n",
      "Epoch 3 training loss: 688.9855312803031\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 47.890918016433716 seconds\n",
      "Epoch 1 training loss: 864.5844435052088\n",
      "Time of epoch 2: 48.767224073410034 seconds\n",
      "Epoch 2 training loss: 699.4964246965762\n",
      "Time of epoch 3: 51.41416835784912 seconds\n",
      "Epoch 3 training loss: 696.2626787461469\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 47.054609060287476 seconds\n",
      "Epoch 1 training loss: 901.8065831846192\n",
      "Time of epoch 2: 51.21129012107849 seconds\n",
      "Epoch 2 training loss: 685.2348373260502\n",
      "Time of epoch 3: 50.188474893569946 seconds\n",
      "Epoch 3 training loss: 684.8906932844477\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 49.36328458786011 seconds\n",
      "Epoch 1 training loss: 679.6624395548502\n",
      "Time of epoch 2: 47.925113916397095 seconds\n",
      "Epoch 2 training loss: 670.3988772474409\n",
      "Time of epoch 3: 48.20545959472656 seconds\n",
      "Epoch 3 training loss: 667.3197939507912\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 49.39996552467346 seconds\n",
      "Epoch 1 training loss: 687.5962928922226\n",
      "Time of epoch 2: 49.53142166137695 seconds\n",
      "Epoch 2 training loss: 679.6521610053669\n",
      "Time of epoch 3: 48.86945462226868 seconds\n",
      "Epoch 3 training loss: 673.9788931945326\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 48.885220527648926 seconds\n",
      "Epoch 1 training loss: 707.1377736304281\n",
      "Time of epoch 2: 49.07868242263794 seconds\n",
      "Epoch 2 training loss: 689.6133308207028\n",
      "Time of epoch 3: 50.02303600311279 seconds\n",
      "Epoch 3 training loss: 687.842986484697\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 50.019635915756226 seconds\n",
      "Epoch 1 training loss: 833.557415715024\n",
      "Time of epoch 2: 48.62953019142151 seconds\n",
      "Epoch 2 training loss: 697.9716253192607\n",
      "Time of epoch 3: 47.88405680656433 seconds\n",
      "Epoch 3 training loss: 695.0728827693811\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 49.43788766860962 seconds\n",
      "Epoch 1 training loss: 925.3320930796076\n",
      "Time of epoch 2: 47.94526505470276 seconds\n",
      "Epoch 2 training loss: 684.885388381579\n",
      "Time of epoch 3: 49.03449058532715 seconds\n",
      "Epoch 3 training loss: 684.8826376176012\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 49.43198919296265 seconds\n",
      "Epoch 1 training loss: 681.9579751892579\n",
      "Time of epoch 2: 49.82030391693115 seconds\n",
      "Epoch 2 training loss: 672.0027596960396\n",
      "Time of epoch 3: 48.044116258621216 seconds\n",
      "Epoch 3 training loss: 667.3336091447569\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 49.25027275085449 seconds\n",
      "Epoch 1 training loss: 684.8201725382498\n",
      "Time of epoch 2: 50.80046081542969 seconds\n",
      "Epoch 2 training loss: 674.3872883553971\n",
      "Time of epoch 3: 49.33930730819702 seconds\n",
      "Epoch 3 training loss: 670.2756615616777\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 48.984190702438354 seconds\n",
      "Epoch 1 training loss: 702.6932636907434\n",
      "Time of epoch 2: 48.74750804901123 seconds\n",
      "Epoch 2 training loss: 686.3608495399945\n",
      "Time of epoch 3: 48.851858377456665 seconds\n",
      "Epoch 3 training loss: 684.2859666824999\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 47.8158860206604 seconds\n",
      "Epoch 1 training loss: 830.1008420462572\n",
      "Time of epoch 2: 47.53863835334778 seconds\n",
      "Epoch 2 training loss: 703.3249676880034\n",
      "Time of epoch 3: 49.53243064880371 seconds\n",
      "Epoch 3 training loss: 700.1890271233286\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 48.174705505371094 seconds\n",
      "Epoch 1 training loss: 1251.984705985888\n",
      "Time of epoch 2: 49.38435196876526 seconds\n",
      "Epoch 2 training loss: 684.8957295140647\n",
      "Time of epoch 3: 47.77442264556885 seconds\n",
      "Epoch 3 training loss: 684.8861648601494\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 47.53891682624817 seconds\n",
      "Epoch 1 training loss: 681.699136648052\n",
      "Time of epoch 2: 48.377785444259644 seconds\n",
      "Epoch 2 training loss: 672.0005076845207\n",
      "Time of epoch 3: 45.42764854431152 seconds\n",
      "Epoch 3 training loss: 668.1799767976241\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 48.43698477745056 seconds\n",
      "Epoch 1 training loss: 691.3912942198676\n",
      "Time of epoch 2: 48.14139413833618 seconds\n",
      "Epoch 2 training loss: 681.4821093011439\n",
      "Time of epoch 3: 49.03731918334961 seconds\n",
      "Epoch 3 training loss: 675.4876445949676\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 48.892640829086304 seconds\n",
      "Epoch 1 training loss: 699.2282822465453\n",
      "Time of epoch 2: 49.43040895462036 seconds\n",
      "Epoch 2 training loss: 684.9468870913225\n",
      "Time of epoch 3: 49.62392282485962 seconds\n",
      "Epoch 3 training loss: 682.5519996093277\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 47.211955070495605 seconds\n",
      "Epoch 1 training loss: 796.7480839367773\n",
      "Time of epoch 2: 48.15389442443848 seconds\n",
      "Epoch 2 training loss: 696.7297787399078\n",
      "Time of epoch 3: 51.07956838607788 seconds\n",
      "Epoch 3 training loss: 694.2270193089604\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 53.89451837539673 seconds\n",
      "Epoch 1 training loss: 1921.167793033508\n",
      "Time of epoch 2: 52.87249803543091 seconds\n",
      "Epoch 2 training loss: 684.8918245642743\n",
      "Time of epoch 3: 52.45106506347656 seconds\n",
      "Epoch 3 training loss: 684.8851314225412\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 53.07365131378174 seconds\n",
      "Epoch 1 training loss: 679.4156673425099\n",
      "Time of epoch 2: 54.0918242931366 seconds\n",
      "Epoch 2 training loss: 670.5235851386788\n",
      "Time of epoch 3: 54.24043297767639 seconds\n",
      "Epoch 3 training loss: 667.3626771914611\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 53.49333381652832 seconds\n",
      "Epoch 1 training loss: 685.9584217216043\n",
      "Time of epoch 2: 52.715932846069336 seconds\n",
      "Epoch 2 training loss: 675.7951265506914\n",
      "Time of epoch 3: 54.07634496688843 seconds\n",
      "Epoch 3 training loss: 670.4781909361224\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 53.64014291763306 seconds\n",
      "Epoch 1 training loss: 703.145254039945\n",
      "Time of epoch 2: 54.97126340866089 seconds\n",
      "Epoch 2 training loss: 688.8760117101629\n",
      "Time of epoch 3: 51.92191243171692 seconds\n",
      "Epoch 3 training loss: 686.1525318768397\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 128, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 54.18058156967163 seconds\n",
      "Epoch 1 training loss: 778.9491982287992\n",
      "Time of epoch 2: 52.01949429512024 seconds\n",
      "Epoch 2 training loss: 696.637879538676\n",
      "Time of epoch 3: 52.95152163505554 seconds\n",
      "Epoch 3 training loss: 693.6212683655026\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 48.03436326980591 seconds\n",
      "Epoch 1 training loss: 1640.3086283914458\n",
      "Time of epoch 2: 48.63621258735657 seconds\n",
      "Epoch 2 training loss: 685.0245622994514\n",
      "Time of epoch 3: 47.30268573760986 seconds\n",
      "Epoch 3 training loss: 684.8836208936639\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 64, lr: 0.01\n",
      "Time of epoch 2: 49.80902862548828 seconds\n",
      "Epoch 2 training loss: 667.6376587779553\n",
      "Time of epoch 3: 47.09210395812988 seconds\n",
      "Epoch 3 training loss: 664.7531894441141\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 47.48009490966797 seconds\n",
      "Epoch 1 training loss: 681.4624502844754\n",
      "Time of epoch 2: 49.39161276817322 seconds\n",
      "Epoch 2 training loss: 671.419233138974\n",
      "Time of epoch 3: 47.71222281455994 seconds\n",
      "Epoch 3 training loss: 667.0237358913128\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 49.21740531921387 seconds\n",
      "Epoch 1 training loss: 696.3220796294073\n",
      "Time of epoch 2: 50.28108549118042 seconds\n",
      "Epoch 2 training loss: 681.7528568096185\n",
      "Time of epoch 3: 48.06769895553589 seconds\n",
      "Epoch 3 training loss: 679.5990928973346\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 47.66220045089722 seconds\n",
      "Epoch 1 training loss: 791.8837153340178\n",
      "Time of epoch 2: 46.76802444458008 seconds\n",
      "Epoch 2 training loss: 694.2286500772074\n",
      "Time of epoch 3: 49.07391405105591 seconds\n",
      "Epoch 3 training loss: 690.7731828147861\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 48.82067370414734 seconds\n",
      "Epoch 1 training loss: 60562.933282860875\n",
      "Time of epoch 2: 47.61159133911133 seconds\n",
      "Epoch 2 training loss: 691.4094878227351\n",
      "Time of epoch 3: 43.2393684387207 seconds\n",
      "Epoch 3 training loss: 684.8772151522085\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 42.55291152000427 seconds\n",
      "Epoch 1 training loss: 677.7838547469521\n",
      "Time of epoch 2: 44.94561195373535 seconds\n",
      "Epoch 2 training loss: 667.5668516347713\n",
      "Time of epoch 3: 45.44112706184387 seconds\n",
      "Epoch 3 training loss: 665.3684010300101\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 45.755592584609985 seconds\n",
      "Epoch 1 training loss: 679.7254383077911\n",
      "Time of epoch 2: 45.11168169975281 seconds\n",
      "Epoch 2 training loss: 669.21249901857\n",
      "Time of epoch 3: 45.34783220291138 seconds\n",
      "Epoch 3 training loss: 665.2350662543844\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 45.401293992996216 seconds\n",
      "Epoch 1 training loss: 695.5048522419615\n",
      "Time of epoch 2: 45.608864068984985 seconds\n",
      "Epoch 2 training loss: 682.3039191619445\n",
      "Time of epoch 3: 42.77367281913757 seconds\n",
      "Epoch 3 training loss: 680.1346793125002\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 45.469019651412964 seconds\n",
      "Epoch 1 training loss: 769.4425305289413\n",
      "Time of epoch 2: 44.437840938568115 seconds\n",
      "Epoch 2 training loss: 690.5378138173934\n",
      "Time of epoch 3: 42.62266802787781 seconds\n",
      "Epoch 3 training loss: 688.2144765103293\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 43.52472972869873 seconds\n",
      "Epoch 1 training loss: 28558.279003483243\n",
      "Time of epoch 2: 43.870614528656006 seconds\n",
      "Epoch 2 training loss: 685.1790231381895\n",
      "Time of epoch 3: 43.592812061309814 seconds\n",
      "Epoch 3 training loss: 684.8831150285201\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 42.87825083732605 seconds\n",
      "Epoch 1 training loss: 676.5944455492644\n",
      "Time of epoch 2: 44.70144271850586 seconds\n",
      "Epoch 2 training loss: 667.8400409282427\n",
      "Time of epoch 3: 46.531599044799805 seconds\n",
      "Epoch 3 training loss: 664.8867661760983\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 45.70326519012451 seconds\n",
      "Epoch 1 training loss: 679.79041496181\n",
      "Time of epoch 2: 44.77435326576233 seconds\n",
      "Epoch 2 training loss: 668.836123175285\n",
      "Time of epoch 3: 46.20507740974426 seconds\n",
      "Epoch 3 training loss: 664.86218076077\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 44.20012927055359 seconds\n",
      "Epoch 1 training loss: 691.8511055336105\n",
      "Time of epoch 2: 45.34149384498596 seconds\n",
      "Epoch 2 training loss: 680.2563813498028\n",
      "Time of epoch 3: 44.57925844192505 seconds\n",
      "Epoch 3 training loss: 678.1089059356485\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 45.43331980705261 seconds\n",
      "Epoch 1 training loss: 755.1531283326918\n",
      "Time of epoch 2: 49.38826537132263 seconds\n",
      "Epoch 2 training loss: 688.8526379627866\n",
      "Time of epoch 3: 48.87249994277954 seconds\n",
      "Epoch 3 training loss: 686.7404491088372\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 49.83714818954468 seconds\n",
      "Epoch 1 training loss: 8882.454421775841\n",
      "Time of epoch 2: 50.406168937683105 seconds\n",
      "Epoch 2 training loss: 685.7199268072301\n",
      "Time of epoch 3: 49.03961801528931 seconds\n",
      "Epoch 3 training loss: 684.8830238977158\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 51.397308588027954 seconds\n",
      "Epoch 1 training loss: 676.8284818772656\n",
      "Time of epoch 2: 50.1239914894104 seconds\n",
      "Epoch 2 training loss: 667.2607133430984\n",
      "Time of epoch 3: 51.4494366645813 seconds\n",
      "Epoch 3 training loss: 664.9114750100904\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 50.21070981025696 seconds\n",
      "Epoch 1 training loss: 678.8325381003799\n",
      "Time of epoch 2: 49.442108154296875 seconds\n",
      "Epoch 2 training loss: 667.929391862217\n",
      "Time of epoch 3: 47.3907310962677 seconds\n",
      "Epoch 3 training loss: 663.2805331994764\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 48.36323547363281 seconds\n",
      "Epoch 1 training loss: 691.5134079765664\n",
      "Time of epoch 2: 50.49957633018494 seconds\n",
      "Epoch 2 training loss: 679.9217715030034\n",
      "Time of epoch 3: 49.456042528152466 seconds\n",
      "Epoch 3 training loss: 677.4990241353393\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 49.36209678649902 seconds\n",
      "Epoch 1 training loss: 749.8876276512394\n",
      "Time of epoch 2: 47.28738975524902 seconds\n",
      "Epoch 2 training loss: 690.7826064611705\n",
      "Time of epoch 3: 50.930476665496826 seconds\n",
      "Epoch 3 training loss: 688.270032930206\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 54.909191370010376 seconds\n",
      "Epoch 1 training loss: 17212.504724778097\n",
      "Time of epoch 2: 53.85706901550293 seconds\n",
      "Epoch 2 training loss: 1765.8979865345923\n",
      "Time of epoch 3: 53.62260961532593 seconds\n",
      "Epoch 3 training loss: 684.8955685707433\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 54.89720153808594 seconds\n",
      "Epoch 1 training loss: 675.7450675918964\n",
      "Time of epoch 2: 55.35025715827942 seconds\n",
      "Epoch 2 training loss: 667.6081089713881\n",
      "Time of epoch 3: 54.08631110191345 seconds\n",
      "Epoch 3 training loss: 665.7897120741999\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 54.59909963607788 seconds\n",
      "Epoch 1 training loss: 677.6360735882095\n",
      "Time of epoch 2: 53.39969205856323 seconds\n",
      "Epoch 2 training loss: 666.7184028311159\n",
      "Time of epoch 3: 54.875327348709106 seconds\n",
      "Epoch 3 training loss: 661.8627428540834\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 54.10288906097412 seconds\n",
      "Epoch 1 training loss: 689.5507622014721\n",
      "Time of epoch 2: 52.713863372802734 seconds\n",
      "Epoch 2 training loss: 678.2181566978865\n",
      "Time of epoch 3: 51.87587285041809 seconds\n",
      "Epoch 3 training loss: 675.0191580526835\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 256, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 51.36567449569702 seconds\n",
      "Epoch 1 training loss: 736.8650117678357\n",
      "Time of epoch 2: 51.9448447227478 seconds\n",
      "Epoch 2 training loss: 688.1601479080895\n",
      "Time of epoch 3: 51.72591042518616 seconds\n",
      "Epoch 3 training loss: 686.5561253512232\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 43.3162145614624 seconds\n",
      "Epoch 1 training loss: 5280.984986270699\n",
      "Time of epoch 2: 43.598814725875854 seconds\n",
      "Epoch 2 training loss: 685.4702901167237\n",
      "Time of epoch 3: 44.15251326560974 seconds\n",
      "Epoch 3 training loss: 684.9239585908765\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 46.191532373428345 seconds\n",
      "Epoch 1 training loss: 673.9182632245748\n",
      "Time of epoch 2: 44.14456486701965 seconds\n",
      "Epoch 2 training loss: 665.8944998498625\n",
      "Time of epoch 3: 44.07685685157776 seconds\n",
      "Epoch 3 training loss: 662.3594626446857\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 43.87175512313843 seconds\n",
      "Epoch 1 training loss: 678.0953504134604\n",
      "Time of epoch 2: 45.14736819267273 seconds\n",
      "Epoch 2 training loss: 667.0104968618735\n",
      "Time of epoch 3: 44.084144115448 seconds\n",
      "Epoch 3 training loss: 662.7179315581222\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 45.925424575805664 seconds\n",
      "Epoch 1 training loss: 690.2882513611546\n",
      "Time of epoch 2: 45.01331377029419 seconds\n",
      "Epoch 2 training loss: 679.0006479657707\n",
      "Time of epoch 3: 43.09980487823486 seconds\n",
      "Epoch 3 training loss: 676.0483757036668\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 45.91335988044739 seconds\n",
      "Epoch 1 training loss: 747.8625948452277\n",
      "Time of epoch 2: 44.25226092338562 seconds\n",
      "Epoch 2 training loss: 688.1449787174303\n",
      "Time of epoch 3: 44.1176860332489 seconds\n",
      "Epoch 3 training loss: 685.8891396965431\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 44.856253147125244 seconds\n",
      "Epoch 1 training loss: 16068.35412274722\n",
      "Time of epoch 2: 44.16709804534912 seconds\n",
      "Epoch 2 training loss: 684.8800106313728\n",
      "Time of epoch 3: 44.416051387786865 seconds\n",
      "Epoch 3 training loss: 685.165679628809\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 45.217620611190796 seconds\n",
      "Epoch 1 training loss: 673.5314327889026\n",
      "Time of epoch 2: 44.373074531555176 seconds\n",
      "Epoch 2 training loss: 665.1124527835603\n",
      "Time of epoch 3: 44.4043345451355 seconds\n",
      "Epoch 3 training loss: 663.3529092933735\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 44.09004497528076 seconds\n",
      "Epoch 1 training loss: 676.2468580406386\n",
      "Time of epoch 2: 43.7034707069397 seconds\n",
      "Epoch 2 training loss: 665.1464996981235\n",
      "Time of epoch 3: 44.51738095283508 seconds\n",
      "Epoch 3 training loss: 661.4029213088237\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 47.7683801651001 seconds\n",
      "Epoch 1 training loss: 687.4605526837928\n",
      "Time of epoch 2: 46.30082154273987 seconds\n",
      "Epoch 2 training loss: 677.6621188046922\n",
      "Time of epoch 3: 47.651758670806885 seconds\n",
      "Epoch 3 training loss: 675.0658783978969\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 44.33074140548706 seconds\n",
      "Epoch 1 training loss: 736.2908756845087\n",
      "Time of epoch 2: 45.772250175476074 seconds\n",
      "Epoch 2 training loss: 687.3992911202341\n",
      "Time of epoch 3: 44.81147885322571 seconds\n",
      "Epoch 3 training loss: 685.1958535912178\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 44.551000356674194 seconds\n",
      "Epoch 1 training loss: 45842.42556988435\n",
      "Time of epoch 2: 49.79269576072693 seconds\n",
      "Epoch 2 training loss: 688.679069917368\n",
      "Time of epoch 3: 51.18069243431091 seconds\n",
      "Epoch 3 training loss: 684.8790298019616\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 49.8983211517334 seconds\n",
      "Epoch 1 training loss: 673.1839042892154\n",
      "Time of epoch 2: 45.33014130592346 seconds\n",
      "Epoch 2 training loss: 664.8800291520773\n",
      "Time of epoch 3: 48.704323053359985 seconds\n",
      "Epoch 3 training loss: 663.208887413507\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 43.30083155632019 seconds\n",
      "Epoch 1 training loss: 675.1990901187424\n",
      "Time of epoch 2: 46.77173733711243 seconds\n",
      "Epoch 2 training loss: 664.2135825026544\n",
      "Time of epoch 3: 47.030415773391724 seconds\n",
      "Epoch 3 training loss: 659.9460066264171\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 46.66408085823059 seconds\n",
      "Epoch 1 training loss: 686.4358842370923\n",
      "Time of epoch 2: 45.75613975524902 seconds\n",
      "Epoch 2 training loss: 676.5233089033064\n",
      "Time of epoch 3: 45.69822692871094 seconds\n",
      "Epoch 3 training loss: 674.3361719944432\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 45.19441342353821 seconds\n",
      "Epoch 1 training loss: 725.1592473806664\n",
      "Time of epoch 2: 44.583786487579346 seconds\n",
      "Epoch 2 training loss: 684.9599184063924\n",
      "Time of epoch 3: 47.66839051246643 seconds\n",
      "Epoch 3 training loss: 683.0170233827714\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 45.196065187454224 seconds\n",
      "Epoch 1 training loss: 104921.38102541116\n",
      "Time of epoch 2: 47.80468797683716 seconds\n",
      "Epoch 2 training loss: 690.9928267886374\n",
      "Time of epoch 3: 50.90610480308533 seconds\n",
      "Epoch 3 training loss: 684.8853030389851\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 52.208313941955566 seconds\n",
      "Epoch 1 training loss: 673.2142597684478\n",
      "Time of epoch 2: 51.098018646240234 seconds\n",
      "Epoch 2 training loss: 665.7892011633326\n",
      "Time of epoch 3: 50.57724165916443 seconds\n",
      "Epoch 3 training loss: 664.4576271775635\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 53.629162073135376 seconds\n",
      "Epoch 1 training loss: 675.1934806981792\n",
      "Time of epoch 2: 50.70204019546509 seconds\n",
      "Epoch 2 training loss: 664.3430867309136\n",
      "Time of epoch 3: 47.244654417037964 seconds\n",
      "Epoch 3 training loss: 659.4935701617856\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 47.73313927650452 seconds\n",
      "Epoch 1 training loss: 684.6555312158051\n",
      "Time of epoch 2: 48.61024355888367 seconds\n",
      "Epoch 2 training loss: 674.8418981900406\n",
      "Time of epoch 3: 48.17576789855957 seconds\n",
      "Epoch 3 training loss: 671.8623853714895\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 44.565064907073975 seconds\n",
      "Epoch 1 training loss: 719.1883213415905\n",
      "Time of epoch 2: 45.95217728614807 seconds\n",
      "Epoch 2 training loss: 683.5384564517395\n",
      "Time of epoch 3: 44.11396503448486 seconds\n",
      "Epoch 3 training loss: 681.7445040462604\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 54.303741216659546 seconds\n",
      "Epoch 1 training loss: 103772.48549017437\n",
      "Time of epoch 2: 57.58037042617798 seconds\n",
      "Epoch 2 training loss: 687.0840463576578\n",
      "Time of epoch 3: 57.099674701690674 seconds\n",
      "Epoch 3 training loss: 684.881089311524\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 59.00747537612915 seconds\n",
      "Epoch 1 training loss: 674.2363195926233\n",
      "Time of epoch 2: 58.27913999557495 seconds\n",
      "Epoch 2 training loss: 667.1594403757563\n",
      "Time of epoch 3: 58.94086694717407 seconds\n",
      "Epoch 3 training loss: 666.7223368870006\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 58.893816232681274 seconds\n",
      "Epoch 1 training loss: 674.3497791925354\n",
      "Time of epoch 2: 57.99337124824524 seconds\n",
      "Epoch 2 training loss: 664.2667952087653\n",
      "Time of epoch 3: 59.36912417411804 seconds\n",
      "Epoch 3 training loss: 660.1228720735803\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 58.48971509933472 seconds\n",
      "Epoch 1 training loss: 683.2358808851191\n",
      "Time of epoch 2: 60.26979947090149 seconds\n",
      "Epoch 2 training loss: 673.1188412515422\n",
      "Time of epoch 3: 58.98392057418823 seconds\n",
      "Epoch 3 training loss: 669.4955451764358\n",
      "-------------------------------\n",
      "batch size: 128, hidden_size: 512, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 58.09305667877197 seconds\n",
      "Epoch 1 training loss: 719.5964986852119\n",
      "Time of epoch 2: 57.86149501800537 seconds\n",
      "Epoch 2 training loss: 685.2666674087839\n",
      "Time of epoch 3: 56.89233040809631 seconds\n",
      "Epoch 3 training loss: 683.5106108219168\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 28.927011966705322 seconds\n",
      "Epoch 1 training loss: 722.4331774515128\n",
      "Time of epoch 2: 32.09704327583313 seconds\n",
      "Epoch 2 training loss: 684.9280919677734\n",
      "Time of epoch 3: 31.829038619995117 seconds\n",
      "Epoch 3 training loss: 684.8813397138649\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 30.44936513900757 seconds\n",
      "Epoch 1 training loss: 690.7982924251387\n",
      "Time of epoch 2: 31.011273622512817 seconds\n",
      "Epoch 2 training loss: 680.1205553778805\n",
      "Time of epoch 3: 29.90172505378723 seconds\n",
      "Epoch 3 training loss: 674.540820099932\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 33.495150566101074 seconds\n",
      "Epoch 1 training loss: 712.3589715915364\n",
      "Time of epoch 2: 33.29405665397644 seconds\n",
      "Epoch 2 training loss: 701.5689598288723\n",
      "Time of epoch 3: 33.95088505744934 seconds\n",
      "Epoch 3 training loss: 699.1356721900527\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 33.15682935714722 seconds\n",
      "Epoch 1 training loss: 753.9048222236339\n",
      "Time of epoch 2: 30.584932565689087 seconds\n",
      "Epoch 2 training loss: 704.373440906135\n",
      "Time of epoch 3: 30.200697660446167 seconds\n",
      "Epoch 3 training loss: 702.3163211803947\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 31.796518325805664 seconds\n",
      "Epoch 1 training loss: 984.7914497067507\n",
      "Time of epoch 2: 32.22951531410217 seconds\n",
      "Epoch 2 training loss: 891.1256262046423\n",
      "Time of epoch 3: 30.979097843170166 seconds\n",
      "Epoch 3 training loss: 726.1567718945394\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 32.802183628082275 seconds\n",
      "Epoch 1 training loss: 702.6076894472602\n",
      "Time of epoch 2: 31.770943880081177 seconds\n",
      "Epoch 2 training loss: 684.8840027577507\n",
      "Time of epoch 3: 31.99593949317932 seconds\n",
      "Epoch 3 training loss: 684.8591413430509\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 31.183175802230835 seconds\n",
      "Epoch 1 training loss: 695.2643843808535\n",
      "Time of epoch 2: 34.44790697097778 seconds\n",
      "Epoch 2 training loss: 682.3932596830574\n",
      "Time of epoch 3: 35.06270980834961 seconds\n",
      "Epoch 3 training loss: 677.555969508079\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 32.88582491874695 seconds\n",
      "Epoch 1 training loss: 710.9745563027133\n",
      "Time of epoch 2: 35.159624338150024 seconds\n",
      "Epoch 2 training loss: 697.8646151670054\n",
      "Time of epoch 3: 34.2173752784729 seconds\n",
      "Epoch 3 training loss: 686.6517703284461\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 35.37823748588562 seconds\n",
      "Epoch 1 training loss: 748.7058382541281\n",
      "Time of epoch 2: 35.13877081871033 seconds\n",
      "Epoch 2 training loss: 703.7393628438933\n",
      "Time of epoch 3: 35.426605224609375 seconds\n",
      "Epoch 3 training loss: 703.0594621252698\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 34.177448987960815 seconds\n",
      "Epoch 1 training loss: 985.8657032371102\n",
      "Time of epoch 2: 33.464537143707275 seconds\n",
      "Epoch 2 training loss: 894.7280804244615\n",
      "Time of epoch 3: 34.43841624259949 seconds\n",
      "Epoch 3 training loss: 725.9959915351918\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 35.11129641532898 seconds\n",
      "Epoch 1 training loss: 731.9827512488114\n",
      "Time of epoch 2: 35.246926069259644 seconds\n",
      "Epoch 2 training loss: 684.8558695348354\n",
      "Time of epoch 3: 33.77012491226196 seconds\n",
      "Epoch 3 training loss: 684.9387800547207\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 33.925753355026245 seconds\n",
      "Epoch 1 training loss: 693.8050587195827\n",
      "Time of epoch 2: 33.77292323112488 seconds\n",
      "Epoch 2 training loss: 682.9719326607947\n",
      "Time of epoch 3: 34.465500831604004 seconds\n",
      "Epoch 3 training loss: 677.0152538722726\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 34.912843465805054 seconds\n",
      "Epoch 1 training loss: 726.5581898686947\n",
      "Time of epoch 2: 34.62569308280945 seconds\n",
      "Epoch 2 training loss: 717.6455016487814\n",
      "Time of epoch 3: 32.431331634521484 seconds\n",
      "Epoch 3 training loss: 713.5432435068827\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 32.51698541641235 seconds\n",
      "Epoch 1 training loss: 729.7009481722687\n",
      "Time of epoch 2: 34.29323673248291 seconds\n",
      "Epoch 2 training loss: 694.8336889916172\n",
      "Time of epoch 3: 33.565396785736084 seconds\n",
      "Epoch 3 training loss: 692.9365624253975\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 32.967429637908936 seconds\n",
      "Epoch 1 training loss: 965.646466589197\n",
      "Time of epoch 2: 36.09754824638367 seconds\n",
      "Epoch 2 training loss: 768.8105706797639\n",
      "Time of epoch 3: 33.63493371009827 seconds\n",
      "Epoch 3 training loss: 705.3593989016305\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 33.263169050216675 seconds\n",
      "Epoch 1 training loss: 740.3460556902227\n",
      "Time of epoch 2: 33.76189994812012 seconds\n",
      "Epoch 2 training loss: 684.8676620427462\n",
      "Time of epoch 3: 32.595540285110474 seconds\n",
      "Epoch 3 training loss: 684.8507565352328\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 34.768609285354614 seconds\n",
      "Epoch 1 training loss: 698.9828757604129\n",
      "Time of epoch 2: 31.131741046905518 seconds\n",
      "Epoch 2 training loss: 686.0669588906866\n",
      "Time of epoch 3: 32.60823345184326 seconds\n",
      "Epoch 3 training loss: 678.4207764445891\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 31.127683877944946 seconds\n",
      "Epoch 1 training loss: 705.2266448488302\n",
      "Time of epoch 2: 34.41100263595581 seconds\n",
      "Epoch 2 training loss: 697.0069984150573\n",
      "Time of epoch 3: 37.06461262702942 seconds\n",
      "Epoch 3 training loss: 693.3182322222177\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 35.37420177459717 seconds\n",
      "Epoch 1 training loss: 741.6383895607213\n",
      "Time of epoch 2: 32.94509983062744 seconds\n",
      "Epoch 2 training loss: 707.257166850252\n",
      "Time of epoch 3: 33.84493350982666 seconds\n",
      "Epoch 3 training loss: 705.9608811522189\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 32.3961923122406 seconds\n",
      "Epoch 1 training loss: 967.3418954808552\n",
      "Time of epoch 2: 32.43081521987915 seconds\n",
      "Epoch 2 training loss: 785.1058951532095\n",
      "Time of epoch 3: 32.15435719490051 seconds\n",
      "Epoch 3 training loss: 713.4857308223297\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 34.12708497047424 seconds\n",
      "Epoch 1 training loss: 844.0455017722567\n",
      "Time of epoch 2: 33.21873879432678 seconds\n",
      "Epoch 2 training loss: 684.8786421765242\n",
      "Time of epoch 3: 34.33870840072632 seconds\n",
      "Epoch 3 training loss: 684.8508618492306\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 35.1409637928009 seconds\n",
      "Epoch 1 training loss: 694.1541925899397\n",
      "Time of epoch 2: 35.11422371864319 seconds\n",
      "Epoch 2 training loss: 682.5948774040311\n",
      "Time of epoch 3: 35.35692548751831 seconds\n",
      "Epoch 3 training loss: 677.7196413460432\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 35.85804891586304 seconds\n",
      "Epoch 1 training loss: 700.9628729816069\n",
      "Time of epoch 2: 32.60481548309326 seconds\n",
      "Epoch 2 training loss: 692.7262847998012\n",
      "Time of epoch 3: 31.93530797958374 seconds\n",
      "Epoch 3 training loss: 685.5690652292396\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 33.94466257095337 seconds\n",
      "Epoch 1 training loss: 720.2925656155611\n",
      "Time of epoch 2: 32.28264760971069 seconds\n",
      "Epoch 2 training loss: 692.1064092550135\n",
      "Time of epoch 3: 32.94869637489319 seconds\n",
      "Epoch 3 training loss: 690.3901215561616\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 64, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 34.84093165397644 seconds\n",
      "Epoch 1 training loss: 946.8564477235154\n",
      "Time of epoch 2: 34.94478249549866 seconds\n",
      "Epoch 2 training loss: 729.4260271035039\n",
      "Time of epoch 3: 35.14343523979187 seconds\n",
      "Epoch 3 training loss: 704.9318170320226\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 32.95830535888672 seconds\n",
      "Epoch 1 training loss: 1318.5024800823755\n",
      "Time of epoch 2: 33.45912408828735 seconds\n",
      "Epoch 2 training loss: 685.6459191579453\n",
      "Time of epoch 3: 33.75225567817688 seconds\n",
      "Epoch 3 training loss: 684.8777501506705\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 32.1293306350708 seconds\n",
      "Epoch 1 training loss: 683.6120648489423\n",
      "Time of epoch 2: 31.660425186157227 seconds\n",
      "Epoch 2 training loss: 673.5301780424114\n",
      "Time of epoch 3: 30.947954893112183 seconds\n",
      "Epoch 3 training loss: 670.7509087025923\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 31.986607551574707 seconds\n",
      "Epoch 1 training loss: 689.8368613520367\n",
      "Time of epoch 2: 34.186062812805176 seconds\n",
      "Epoch 2 training loss: 681.3266829520794\n",
      "Time of epoch 3: 33.41914176940918 seconds\n",
      "Epoch 3 training loss: 679.0210257778549\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 32.11874318122864 seconds\n",
      "Epoch 1 training loss: 725.8617684916776\n",
      "Time of epoch 2: 31.426543951034546 seconds\n",
      "Epoch 2 training loss: 692.8507564034975\n",
      "Time of epoch 3: 32.120983839035034 seconds\n",
      "Epoch 3 training loss: 690.7990513715108\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 29.204941511154175 seconds\n",
      "Epoch 1 training loss: 950.885234909738\n",
      "Time of epoch 2: 32.670989751815796 seconds\n",
      "Epoch 2 training loss: 723.2287163850874\n",
      "Time of epoch 3: 28.653372287750244 seconds\n",
      "Epoch 3 training loss: 695.6971212694141\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 31.904732704162598 seconds\n",
      "Epoch 1 training loss: 915.5169992875708\n",
      "Time of epoch 2: 28.196879148483276 seconds\n",
      "Epoch 2 training loss: 685.5825785017711\n",
      "Time of epoch 3: 30.425001859664917 seconds\n",
      "Epoch 3 training loss: 684.8603149226442\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 33.69353246688843 seconds\n",
      "Epoch 1 training loss: 684.3726666845142\n",
      "Time of epoch 2: 32.72209072113037 seconds\n",
      "Epoch 2 training loss: 674.1072322130007\n",
      "Time of epoch 3: 33.01053547859192 seconds\n",
      "Epoch 3 training loss: 669.0018606249839\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 32.91067409515381 seconds\n",
      "Epoch 1 training loss: 690.9911017011315\n",
      "Time of epoch 2: 33.84923529624939 seconds\n",
      "Epoch 2 training loss: 680.7529996048949\n",
      "Time of epoch 3: 32.59736752510071 seconds\n",
      "Epoch 3 training loss: 675.4743653854006\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 32.70281910896301 seconds\n",
      "Epoch 1 training loss: 717.7465152749753\n",
      "Time of epoch 2: 32.25841164588928 seconds\n",
      "Epoch 2 training loss: 689.121276886808\n",
      "Time of epoch 3: 32.30279755592346 seconds\n",
      "Epoch 3 training loss: 687.3800528885388\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 32.664819955825806 seconds\n",
      "Epoch 1 training loss: 964.2888902481377\n",
      "Time of epoch 2: 31.980482816696167 seconds\n",
      "Epoch 2 training loss: 753.9401678826608\n",
      "Time of epoch 3: 31.74841856956482 seconds\n",
      "Epoch 3 training loss: 705.2201624050418\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 30.532907485961914 seconds\n",
      "Epoch 1 training loss: 900.7210186666529\n",
      "Time of epoch 2: 29.76089906692505 seconds\n",
      "Epoch 2 training loss: 685.7369767564261\n",
      "Time of epoch 3: 33.12956523895264 seconds\n",
      "Epoch 3 training loss: 684.8534265501573\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 31.470798015594482 seconds\n",
      "Epoch 1 training loss: 681.5676601357292\n",
      "Time of epoch 2: 31.41474223136902 seconds\n",
      "Epoch 2 training loss: 671.3989090171012\n",
      "Time of epoch 3: 31.360060930252075 seconds\n",
      "Epoch 3 training loss: 667.0739656970493\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 32.362826108932495 seconds\n",
      "Epoch 1 training loss: 688.5735374372535\n",
      "Time of epoch 2: 31.03917646408081 seconds\n",
      "Epoch 2 training loss: 678.1707459211003\n",
      "Time of epoch 3: 33.169172525405884 seconds\n",
      "Epoch 3 training loss: 672.8296275457835\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 30.93215012550354 seconds\n",
      "Epoch 1 training loss: 717.9896406217572\n",
      "Time of epoch 2: 30.92168664932251 seconds\n",
      "Epoch 2 training loss: 689.7940391464456\n",
      "Time of epoch 3: 32.33720397949219 seconds\n",
      "Epoch 3 training loss: 687.7202190497985\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 33.8178391456604 seconds\n",
      "Epoch 1 training loss: 891.603554731735\n",
      "Time of epoch 2: 32.00212216377258 seconds\n",
      "Epoch 2 training loss: 699.1509563784597\n",
      "Time of epoch 3: 32.738853216171265 seconds\n",
      "Epoch 3 training loss: 693.8716077150284\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 33.95023965835571 seconds\n",
      "Epoch 1 training loss: 2154.5026457678914\n",
      "Time of epoch 2: 32.06249475479126 seconds\n",
      "Epoch 2 training loss: 689.718787924652\n",
      "Time of epoch 3: 33.33150362968445 seconds\n",
      "Epoch 3 training loss: 684.9379332889787\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 33.35961937904358 seconds\n",
      "Epoch 1 training loss: 681.5380935317191\n",
      "Time of epoch 2: 31.61502242088318 seconds\n",
      "Epoch 2 training loss: 671.3677785477282\n",
      "Time of epoch 3: 30.917129516601562 seconds\n",
      "Epoch 3 training loss: 667.158956149056\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 32.17489552497864 seconds\n",
      "Epoch 1 training loss: 694.8647206063715\n",
      "Time of epoch 2: 32.584580183029175 seconds\n",
      "Epoch 2 training loss: 685.6001166156308\n",
      "Time of epoch 3: 32.04012846946716 seconds\n",
      "Epoch 3 training loss: 680.2448838643235\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 31.246869325637817 seconds\n",
      "Epoch 1 training loss: 715.4223182707135\n",
      "Time of epoch 2: 32.556063652038574 seconds\n",
      "Epoch 2 training loss: 691.1021479598312\n",
      "Time of epoch 3: 31.973876476287842 seconds\n",
      "Epoch 3 training loss: 688.6572249061355\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 31.036064863204956 seconds\n",
      "Epoch 1 training loss: 892.5884361083881\n",
      "Time of epoch 2: 30.584746837615967 seconds\n",
      "Epoch 2 training loss: 701.87463183546\n",
      "Time of epoch 3: 30.37048077583313 seconds\n",
      "Epoch 3 training loss: 697.6397791858587\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 32.75721502304077 seconds\n",
      "Epoch 1 training loss: 2182.995663610755\n",
      "Time of epoch 2: 32.973859786987305 seconds\n",
      "Epoch 2 training loss: 685.3249841543209\n",
      "Time of epoch 3: 32.30380296707153 seconds\n",
      "Epoch 3 training loss: 684.8634077578478\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 29.85483193397522 seconds\n",
      "Epoch 1 training loss: 681.9459609512932\n",
      "Time of epoch 2: 30.158538818359375 seconds\n",
      "Epoch 2 training loss: 671.6063432744866\n",
      "Time of epoch 3: 31.668578624725342 seconds\n",
      "Epoch 3 training loss: 667.7257190564424\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 33.82300877571106 seconds\n",
      "Epoch 1 training loss: 693.0244869159669\n",
      "Time of epoch 2: 31.949742078781128 seconds\n",
      "Epoch 2 training loss: 684.7612102942524\n",
      "Time of epoch 3: 32.23119902610779 seconds\n",
      "Epoch 3 training loss: 677.5497968953747\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 32.532978534698486 seconds\n",
      "Epoch 1 training loss: 708.5626059026335\n",
      "Time of epoch 2: 34.80747675895691 seconds\n",
      "Epoch 2 training loss: 688.1544929685856\n",
      "Time of epoch 3: 33.24257516860962 seconds\n",
      "Epoch 3 training loss: 686.0632338798298\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 128, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 33.24594593048096 seconds\n",
      "Epoch 1 training loss: 847.1435686792673\n",
      "Time of epoch 2: 31.503093481063843 seconds\n",
      "Epoch 2 training loss: 699.153378762312\n",
      "Time of epoch 3: 32.30598425865173 seconds\n",
      "Epoch 3 training loss: 695.9093698895814\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 30.52386474609375 seconds\n",
      "Epoch 1 training loss: 2122.0778302912313\n",
      "Time of epoch 2: 31.17291522026062 seconds\n",
      "Epoch 2 training loss: 689.827938831103\n",
      "Time of epoch 3: 34.88776350021362 seconds\n",
      "Epoch 3 training loss: 684.8864483680096\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 34.28233218193054 seconds\n",
      "Epoch 1 training loss: 675.5877794685903\n",
      "Time of epoch 2: 34.24800252914429 seconds\n",
      "Epoch 2 training loss: 667.0400372920129\n",
      "Time of epoch 3: 33.82517218589783 seconds\n",
      "Epoch 3 training loss: 663.5231384513179\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 35.379006147384644 seconds\n",
      "Epoch 1 training loss: 683.751876348975\n",
      "Time of epoch 2: 34.77102708816528 seconds\n",
      "Epoch 2 training loss: 673.385729285598\n",
      "Time of epoch 3: 35.23152756690979 seconds\n",
      "Epoch 3 training loss: 668.5752966785517\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 34.462963342666626 seconds\n",
      "Epoch 1 training loss: 710.2494699749509\n",
      "Time of epoch 2: 34.14815926551819 seconds\n",
      "Epoch 2 training loss: 686.2681581197508\n",
      "Time of epoch 3: 34.03209376335144 seconds\n",
      "Epoch 3 training loss: 684.1073754902258\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 33.49782133102417 seconds\n",
      "Epoch 1 training loss: 875.5690220383722\n",
      "Time of epoch 2: 35.846752643585205 seconds\n",
      "Epoch 2 training loss: 695.9136013587905\n",
      "Time of epoch 3: 35.503111362457275 seconds\n",
      "Epoch 3 training loss: 692.0621718955654\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 34.42907476425171 seconds\n",
      "Epoch 1 training loss: 3108.9855654652115\n",
      "Time of epoch 2: 35.47448801994324 seconds\n",
      "Epoch 2 training loss: 691.6504364895887\n",
      "Time of epoch 3: 33.46601104736328 seconds\n",
      "Epoch 3 training loss: 685.0837944315776\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 32.87693548202515 seconds\n",
      "Epoch 1 training loss: 676.5312025021527\n",
      "Time of epoch 2: 32.88044500350952 seconds\n",
      "Epoch 2 training loss: 667.3810667248528\n",
      "Time of epoch 3: 33.587260484695435 seconds\n",
      "Epoch 3 training loss: 663.832636571401\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 35.42711520195007 seconds\n",
      "Epoch 1 training loss: 684.0581294259337\n",
      "Time of epoch 2: 34.46474552154541 seconds\n",
      "Epoch 2 training loss: 675.8266467349689\n",
      "Time of epoch 3: 33.63680577278137 seconds\n",
      "Epoch 3 training loss: 668.9722295689371\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 34.478931188583374 seconds\n",
      "Epoch 1 training loss: 701.3624229626105\n",
      "Time of epoch 2: 34.97528624534607 seconds\n",
      "Epoch 2 training loss: 682.0561988305703\n",
      "Time of epoch 3: 31.550567626953125 seconds\n",
      "Epoch 3 training loss: 679.8437950189582\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 31.344884395599365 seconds\n",
      "Epoch 1 training loss: 833.0393402392363\n",
      "Time of epoch 2: 32.56700253486633 seconds\n",
      "Epoch 2 training loss: 693.3920432569267\n",
      "Time of epoch 3: 32.372519969940186 seconds\n",
      "Epoch 3 training loss: 689.4876762603371\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 35.07697510719299 seconds\n",
      "Epoch 1 training loss: 7546.107305371988\n",
      "Time of epoch 2: 31.848713159561157 seconds\n",
      "Epoch 2 training loss: 695.6250586807888\n",
      "Time of epoch 3: 34.2677435874939 seconds\n",
      "Epoch 3 training loss: 685.600873443894\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 32.29000997543335 seconds\n",
      "Epoch 1 training loss: 677.1761156548674\n",
      "Time of epoch 2: 32.40384912490845 seconds\n",
      "Epoch 2 training loss: 668.0402267259857\n",
      "Time of epoch 3: 32.26492238044739 seconds\n",
      "Epoch 3 training loss: 664.3869088604621\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 32.23621225357056 seconds\n",
      "Epoch 1 training loss: 683.176770026472\n",
      "Time of epoch 2: 29.53848171234131 seconds\n",
      "Epoch 2 training loss: 671.6626969057016\n",
      "Time of epoch 3: 31.374443531036377 seconds\n",
      "Epoch 3 training loss: 667.265431530003\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 28.581602096557617 seconds\n",
      "Epoch 1 training loss: 698.4595798689779\n",
      "Time of epoch 2: 29.51402497291565 seconds\n",
      "Epoch 2 training loss: 680.762047153335\n",
      "Time of epoch 3: 30.085814952850342 seconds\n",
      "Epoch 3 training loss: 678.449427533736\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 30.521893739700317 seconds\n",
      "Epoch 1 training loss: 814.4082797094654\n",
      "Time of epoch 2: 29.111757516860962 seconds\n",
      "Epoch 2 training loss: 691.1767909693498\n",
      "Time of epoch 3: 29.84220004081726 seconds\n",
      "Epoch 3 training loss: 688.5756888098207\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 29.50238847732544 seconds\n",
      "Epoch 1 training loss: 13703.395687682474\n",
      "Time of epoch 2: 29.842897176742554 seconds\n",
      "Epoch 2 training loss: 699.6942586888538\n",
      "Time of epoch 3: 29.791497707366943 seconds\n",
      "Epoch 3 training loss: 686.1254494440445\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 32.80933594703674 seconds\n",
      "Epoch 1 training loss: 676.1666505498279\n",
      "Time of epoch 2: 32.43703603744507 seconds\n",
      "Epoch 2 training loss: 666.6685555115334\n",
      "Time of epoch 3: 33.629175424575806 seconds\n",
      "Epoch 3 training loss: 664.0009771417583\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 32.64406776428223 seconds\n",
      "Epoch 1 training loss: 681.0758107885663\n",
      "Time of epoch 2: 32.32469201087952 seconds\n",
      "Epoch 2 training loss: 670.1945439200298\n",
      "Time of epoch 3: 33.06997013092041 seconds\n",
      "Epoch 3 training loss: 665.5369863485745\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 31.203081607818604 seconds\n",
      "Epoch 1 training loss: 697.5334999781743\n",
      "Time of epoch 2: 30.83160376548767 seconds\n",
      "Epoch 2 training loss: 679.7754774331385\n",
      "Time of epoch 3: 30.627477645874023 seconds\n",
      "Epoch 3 training loss: 676.9865489369433\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 31.976492404937744 seconds\n",
      "Epoch 1 training loss: 790.2896050715483\n",
      "Time of epoch 2: 31.029044151306152 seconds\n",
      "Epoch 2 training loss: 689.9628121557614\n",
      "Time of epoch 3: 31.525269508361816 seconds\n",
      "Epoch 3 training loss: 687.3888705732646\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 31.28032684326172 seconds\n",
      "Epoch 1 training loss: 27878.200562482885\n",
      "Time of epoch 2: 31.15125799179077 seconds\n",
      "Epoch 2 training loss: 802.6324391849241\n",
      "Time of epoch 3: 32.94915461540222 seconds\n",
      "Epoch 3 training loss: 685.1596519154078\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 32.448281049728394 seconds\n",
      "Epoch 1 training loss: 677.5098759778233\n",
      "Time of epoch 2: 32.39205837249756 seconds\n",
      "Epoch 2 training loss: 667.7765011126307\n",
      "Time of epoch 3: 32.32323098182678 seconds\n",
      "Epoch 3 training loss: 664.2462823829735\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 33.34763312339783 seconds\n",
      "Epoch 1 training loss: 681.7459851410367\n",
      "Time of epoch 2: 35.364768266677856 seconds\n",
      "Epoch 2 training loss: 671.5211100424137\n",
      "Time of epoch 3: 35.87285780906677 seconds\n",
      "Epoch 3 training loss: 666.2574043450138\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 35.30189657211304 seconds\n",
      "Epoch 1 training loss: 698.3638599570562\n",
      "Time of epoch 2: 35.276416540145874 seconds\n",
      "Epoch 2 training loss: 680.7114278180776\n",
      "Time of epoch 3: 35.414536476135254 seconds\n",
      "Epoch 3 training loss: 677.9462427924707\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 256, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 35.3271689414978 seconds\n",
      "Epoch 1 training loss: 780.6235706523881\n",
      "Time of epoch 2: 33.28670001029968 seconds\n",
      "Epoch 2 training loss: 690.8653697711317\n",
      "Time of epoch 3: 33.002557039260864 seconds\n",
      "Epoch 3 training loss: 687.9585776258931\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 64, lr: 0.5\n",
      "Time of epoch 1: 31.924522876739502 seconds\n",
      "Epoch 1 training loss: 15198.844579722392\n",
      "Time of epoch 2: 33.7002112865448 seconds\n",
      "Epoch 2 training loss: 702.0872019923704\n",
      "Time of epoch 3: 31.784736156463623 seconds\n",
      "Epoch 3 training loss: 686.1660157556948\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 64, lr: 0.01\n",
      "Time of epoch 1: 33.42778277397156 seconds\n",
      "Epoch 1 training loss: 673.7914700550708\n",
      "Time of epoch 2: 32.504234313964844 seconds\n",
      "Epoch 2 training loss: 664.1971743350692\n",
      "Time of epoch 3: 34.67536950111389 seconds\n",
      "Epoch 3 training loss: 661.415405301555\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 64, lr: 0.001\n",
      "Time of epoch 1: 33.96006774902344 seconds\n",
      "Epoch 1 training loss: 679.2689345417996\n",
      "Time of epoch 2: 32.564138412475586 seconds\n",
      "Epoch 2 training loss: 667.3919408075147\n",
      "Time of epoch 3: 32.08126521110535 seconds\n",
      "Epoch 3 training loss: 663.0670410856862\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 64, lr: 0.0001\n",
      "Time of epoch 1: 34.74982953071594 seconds\n",
      "Epoch 1 training loss: 698.2246712052176\n",
      "Time of epoch 2: 34.68706727027893 seconds\n",
      "Epoch 2 training loss: 680.6761023094577\n",
      "Time of epoch 3: 35.56801080703735 seconds\n",
      "Epoch 3 training loss: 678.2420426639596\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 64, lr: 1e-05\n",
      "Time of epoch 1: 32.83261728286743 seconds\n",
      "Epoch 1 training loss: 791.7742423675542\n",
      "Time of epoch 2: 33.19282603263855 seconds\n",
      "Epoch 2 training loss: 691.057247409214\n",
      "Time of epoch 3: 34.867563247680664 seconds\n",
      "Epoch 3 training loss: 687.4771596041575\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 128, lr: 0.5\n",
      "Time of epoch 1: 36.334683656692505 seconds\n",
      "Epoch 1 training loss: 15841.95437126892\n",
      "Time of epoch 2: 32.95313811302185 seconds\n",
      "Epoch 2 training loss: 711.5802751212293\n",
      "Time of epoch 3: 34.15521812438965 seconds\n",
      "Epoch 3 training loss: 684.8842307951802\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 128, lr: 0.01\n",
      "Time of epoch 1: 34.28145956993103 seconds\n",
      "Epoch 1 training loss: 673.6615565325783\n",
      "Time of epoch 2: 34.13098359107971 seconds\n",
      "Epoch 2 training loss: 664.6928498359808\n",
      "Time of epoch 3: 35.64499855041504 seconds\n",
      "Epoch 3 training loss: 661.6447034376728\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 128, lr: 0.001\n",
      "Time of epoch 1: 34.810179233551025 seconds\n",
      "Epoch 1 training loss: 677.788877941426\n",
      "Time of epoch 2: 34.80390429496765 seconds\n",
      "Epoch 2 training loss: 666.3684946469111\n",
      "Time of epoch 3: 34.891205072402954 seconds\n",
      "Epoch 3 training loss: 661.8538459382166\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 128, lr: 0.0001\n",
      "Time of epoch 1: 33.80142426490784 seconds\n",
      "Epoch 1 training loss: 694.7869504289702\n",
      "Time of epoch 2: 33.14118432998657 seconds\n",
      "Epoch 2 training loss: 679.5115995264935\n",
      "Time of epoch 3: 33.43404245376587 seconds\n",
      "Epoch 3 training loss: 677.121701662426\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 128, lr: 1e-05\n",
      "Time of epoch 1: 33.535598039627075 seconds\n",
      "Epoch 1 training loss: 773.8920374431993\n",
      "Time of epoch 2: 33.17982578277588 seconds\n",
      "Epoch 2 training loss: 687.4810958180828\n",
      "Time of epoch 3: 34.955843925476074 seconds\n",
      "Epoch 3 training loss: 685.0433899602048\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 256, lr: 0.5\n",
      "Time of epoch 1: 34.69627833366394 seconds\n",
      "Epoch 1 training loss: 69409.51531020249\n",
      "Time of epoch 2: 34.34873580932617 seconds\n",
      "Epoch 2 training loss: 701.3373399971166\n",
      "Time of epoch 3: 33.307016134262085 seconds\n",
      "Epoch 3 training loss: 692.2077419628461\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 256, lr: 0.01\n",
      "Time of epoch 1: 34.021366119384766 seconds\n",
      "Epoch 1 training loss: 673.6530395473982\n",
      "Time of epoch 2: 33.3943247795105 seconds\n",
      "Epoch 2 training loss: 664.6935321081459\n",
      "Time of epoch 3: 31.795627117156982 seconds\n",
      "Epoch 3 training loss: 661.4238123649097\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 256, lr: 0.001\n",
      "Time of epoch 1: 35.209975481033325 seconds\n",
      "Epoch 1 training loss: 677.6768936094617\n",
      "Time of epoch 2: 36.1023211479187 seconds\n",
      "Epoch 2 training loss: 666.1076927415243\n",
      "Time of epoch 3: 35.93613362312317 seconds\n",
      "Epoch 3 training loss: 661.2931490984737\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 256, lr: 0.0001\n",
      "Time of epoch 1: 35.14473533630371 seconds\n",
      "Epoch 1 training loss: 692.8743190950166\n",
      "Time of epoch 2: 35.320313453674316 seconds\n",
      "Epoch 2 training loss: 678.5444607649741\n",
      "Time of epoch 3: 34.49554109573364 seconds\n",
      "Epoch 3 training loss: 675.7886773705445\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 256, lr: 1e-05\n",
      "Time of epoch 1: 34.732951641082764 seconds\n",
      "Epoch 1 training loss: 756.0083617111602\n",
      "Time of epoch 2: 35.161725759506226 seconds\n",
      "Epoch 2 training loss: 686.8566143707146\n",
      "Time of epoch 3: 34.62440848350525 seconds\n",
      "Epoch 3 training loss: 684.6646475810154\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 512, lr: 0.5\n",
      "Time of epoch 1: 34.21437454223633 seconds\n",
      "Epoch 1 training loss: 100384.3218718056\n",
      "Time of epoch 2: 35.07501554489136 seconds\n",
      "Epoch 2 training loss: 695.3009805461378\n",
      "Time of epoch 3: 36.11455011367798 seconds\n",
      "Epoch 3 training loss: 691.9562189276243\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 512, lr: 0.01\n",
      "Time of epoch 1: 35.92313313484192 seconds\n",
      "Epoch 1 training loss: 673.8655402020647\n",
      "Time of epoch 2: 33.81339907646179 seconds\n",
      "Epoch 2 training loss: 664.2995666906478\n",
      "Time of epoch 3: 33.4744598865509 seconds\n",
      "Epoch 3 training loss: 661.1084860444867\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 512, lr: 0.001\n",
      "Time of epoch 1: 34.032999992370605 seconds\n",
      "Epoch 1 training loss: 675.8935412708114\n",
      "Time of epoch 2: 34.26977777481079 seconds\n",
      "Epoch 2 training loss: 664.5934965076939\n",
      "Time of epoch 3: 33.60481095314026 seconds\n",
      "Epoch 3 training loss: 660.2052994855675\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 512, lr: 0.0001\n",
      "Time of epoch 1: 34.037071228027344 seconds\n",
      "Epoch 1 training loss: 690.2078625392601\n",
      "Time of epoch 2: 33.683356285095215 seconds\n",
      "Epoch 2 training loss: 676.7283473802773\n",
      "Time of epoch 3: 33.377585887908936 seconds\n",
      "Epoch 3 training loss: 674.0977510199649\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 512, lr: 1e-05\n",
      "Time of epoch 1: 33.62369084358215 seconds\n",
      "Epoch 1 training loss: 749.1332023099072\n",
      "Time of epoch 2: 33.17251706123352 seconds\n",
      "Epoch 2 training loss: 685.5926822537125\n",
      "Time of epoch 3: 34.47893571853638 seconds\n",
      "Epoch 3 training loss: 683.7186683658309\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 1024, lr: 0.5\n",
      "Time of epoch 1: 35.00896406173706 seconds\n",
      "Epoch 1 training loss: 173118.0310426647\n",
      "Time of epoch 2: 33.39661645889282 seconds\n",
      "Epoch 2 training loss: 709.3517731576335\n",
      "Time of epoch 3: 32.78970670700073 seconds\n",
      "Epoch 3 training loss: 698.5345482817094\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 1024, lr: 0.01\n",
      "Time of epoch 1: 34.0700466632843 seconds\n",
      "Epoch 1 training loss: 673.9412832108733\n",
      "Time of epoch 2: 35.78945851325989 seconds\n",
      "Epoch 2 training loss: 664.7037227715792\n",
      "Time of epoch 3: 34.36073350906372 seconds\n",
      "Epoch 3 training loss: 662.3976300723671\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 1024, lr: 0.001\n",
      "Time of epoch 1: 35.83397150039673 seconds\n",
      "Epoch 1 training loss: 676.7568958141877\n",
      "Time of epoch 2: 35.91510033607483 seconds\n",
      "Epoch 2 training loss: 665.6741260404348\n",
      "Time of epoch 3: 35.26370286941528 seconds\n",
      "Epoch 3 training loss: 661.4506216494977\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 1024, lr: 0.0001\n",
      "Time of epoch 1: 34.30822968482971 seconds\n",
      "Epoch 1 training loss: 688.8787643749095\n",
      "Time of epoch 2: 33.8356294631958 seconds\n",
      "Epoch 2 training loss: 675.4185109471923\n",
      "Time of epoch 3: 35.133854389190674 seconds\n",
      "Epoch 3 training loss: 672.1895326636907\n",
      "-------------------------------\n",
      "batch size: 256, hidden_size: 512, embed: 1024, lr: 1e-05\n",
      "Time of epoch 1: 34.098873138427734 seconds\n",
      "Epoch 1 training loss: 740.6824500070819\n",
      "Time of epoch 2: 33.835997104644775 seconds\n",
      "Epoch 2 training loss: 685.3385365479085\n",
      "Time of epoch 3: 32.69111776351929 seconds\n",
      "Epoch 3 training loss: 683.2824348732503\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train, torch.from_numpy(Y_train.values))\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch, shuffle=True)\n",
    "    \n",
    "    for hidden_size in hiddens:\n",
    "        for embed_size in embedding_sizes:\n",
    "            for learning_rate in lrs:\n",
    "\n",
    "                print(\"-------------------------------\")\n",
    "                print(f'batch size: {batch}, hidden_size: {hidden_size}, embed: {embed_size}, lr: {learning_rate}')\n",
    "\n",
    "                model = DeepModel(input_size, embed_size,\n",
    "                                   hidden_size).to(device)\n",
    "\n",
    "                loss_fn = nn.MSELoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "                for epoch in range(3):\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    running_loss = 0\n",
    "                    n = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # Loop over batches in an epoch using DataLoader\n",
    "                    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "\n",
    "                        x_batch = x_batch.to(device).long()\n",
    "                        y_batch = y_batch.to(device)\n",
    "\n",
    "                        y_batch_pred = model(x_batch)\n",
    "\n",
    "\n",
    "                        loss = loss_fn(y_batch_pred, y_batch.float())\n",
    "                        running_loss += loss.item()\n",
    "                        n += 1\n",
    "\n",
    "                        # backwards steps\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    print(f'Time of epoch {epoch + 1}: {time.time() - start_time} seconds')\n",
    "\n",
    "                    print(f\"Epoch {epoch + 1} training loss: {np.sqrt(running_loss/n)}\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d462577",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "155eaa6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111/405390511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = DeepModel(input_size, embedding_size,\n\u001b[0m\u001b[1;32m      6\u001b[0m                    hidden_size).to(device)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "hidden_size = 100 \n",
    "embedding_size = 100 #hyperparam\n",
    "\n",
    "\n",
    "model = DeepModel(input_size, embedding_size,\n",
    "                   hidden_size).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3) #can try 1e-4, similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ed20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Time of epoch 1: 74.2941210269928 seconds\n",
      "Epoch 1 training loss: 681.3599767631298\n",
      "-------------------------------\n",
      "Time of epoch 2: 72.87142610549927 seconds\n",
      "Epoch 2 training loss: 671.6922517087262\n",
      "-------------------------------\n",
      "Time of epoch 3: 75.93565511703491 seconds\n",
      "Epoch 3 training loss: 666.0149666913167\n",
      "-------------------------------\n",
      "Time of epoch 4: 73.429612159729 seconds\n",
      "Epoch 4 training loss: 662.8932767960531\n",
      "-------------------------------\n",
      "Time of epoch 5: 72.13198018074036 seconds\n",
      "Epoch 5 training loss: 660.631775611726\n",
      "-------------------------------\n",
      "Time of epoch 6: 71.080570936203 seconds\n",
      "Epoch 6 training loss: 658.1861592989904\n",
      "-------------------------------\n",
      "Time of epoch 7: 70.14974236488342 seconds\n",
      "Epoch 7 training loss: 656.1145735554683\n",
      "-------------------------------\n",
      "Time of epoch 8: 71.7743752002716 seconds\n",
      "Epoch 8 training loss: 654.0231699792648\n",
      "-------------------------------\n",
      "Time of epoch 9: 68.14510536193848 seconds\n",
      "Epoch 9 training loss: 653.0246822406423\n",
      "-------------------------------\n",
      "Time of epoch 10: 69.85725331306458 seconds\n",
      "Epoch 10 training loss: 651.9930766204425\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "valid_losses = []\n",
    "for epoch in range(5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"-------------------------------\")\n",
    "    \n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device).long()\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        y_batch_pred = model(x_batch)\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_batch_pred, y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Time of epoch {epoch + 1}: {time.time() - start_time} seconds')\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} training loss: {np.sqrt(running_loss/n)}\")\n",
    "    \n",
    "    \n",
    "    losses.append(np.sqrt(running_loss/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b7355",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489118b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694.7105"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds = model(X_valid.long().to(device))\n",
    "np.sqrt(\n",
    "    F.mse_loss(val_preds.cpu(), \n",
    "               torch.Tensor(Y_valid.values)).detach().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f2ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4829c",
   "metadata": {},
   "source": [
    "# Prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6783f98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95152c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    test_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")\n",
    "\n",
    "#making interaction features\n",
    "test_data['STAND_HR'] = test_data['ORIGIN_STAND'].astype(str) + '_' + test_data['HR'].astype(str)\n",
    "test_data['STAND_DAY'] = test_data['DAY'].astype(str) + '_' + test_data['ORIGIN_STAND'].astype(str)\n",
    "test_data['DAY_HR'] = test_data['DAY'].astype(str) + '_' + test_data['HR'].astype(str)\n",
    "\n",
    "#time cols\n",
    "test_data['TOD'] = pd.cut(test_data['HR'], bins=[0, 12, 16, 20, 24], labels=[1, 2, 3, 4], right=False)\n",
    "test_data['FROM_NOON'] = test_data['HR'] - 12 #note that this column is continuous\n",
    "\n",
    "embed_cols = (['YR', 'MON', 'DAY', 'HR', 'WK',\n",
    "               'ORIGIN_STAND', 'CALL_TYPE', 'TAXI_ID',\n",
    "              'STAND_HR', 'STAND_DAY', 'DAY_HR', \"TOD\"])\n",
    "\n",
    "for column in embed_cols:\n",
    "    test_data[column] = test_data[column].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1898e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \n",
    "                 'ORIGIN_STAND', 'CALL_TYPE', \n",
    "                 'TAXI_ID', 'STAND_HR', 'STAND_DAY', \n",
    "                 'DAY_HR', \"TOD\"]]\n",
    "\n",
    "X_test = torch.Tensor(X_test.to_numpy()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "242b16df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>YR</th>\n",
       "      <th>MON</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HR</th>\n",
       "      <th>WK</th>\n",
       "      <th>STAND_HR</th>\n",
       "      <th>STAND_DAY</th>\n",
       "      <th>DAY_HR</th>\n",
       "      <th>TOD</th>\n",
       "      <th>FROM_NOON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>190</td>\n",
       "      <td>1408039037</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "      <td>1408038611</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>129</td>\n",
       "      <td>1408038568</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>171</td>\n",
       "      <td>1408039090</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>217</td>\n",
       "      <td>1408039177</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>T323</td>\n",
       "      <td>0</td>\n",
       "      <td>70885.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>148</td>\n",
       "      <td>1419171485</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>T324</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>1419170802</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>T325</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>72</td>\n",
       "      <td>1419172121</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>T326</td>\n",
       "      <td>0</td>\n",
       "      <td>76232.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>229</td>\n",
       "      <td>1419171980</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>T327</td>\n",
       "      <td>0</td>\n",
       "      <td>31208.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>86</td>\n",
       "      <td>1419171420</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TRIP_ID  CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID   TIMESTAMP  \\\n",
       "0        T1          1          NaN            10      190  1408039037   \n",
       "1        T2          1          NaN            33       37  1408038611   \n",
       "2        T3          1          NaN            10      129  1408038568   \n",
       "3        T4          1          NaN            30      171  1408039090   \n",
       "4        T5          1          NaN            13      217  1408039177   \n",
       "..      ...        ...          ...           ...      ...         ...   \n",
       "315    T323          0      70885.0            -1      148  1419171485   \n",
       "316    T324          1          NaN            30        8  1419170802   \n",
       "317    T325          2          NaN            -1       72  1419172121   \n",
       "318    T326          0      76232.0            -1      229  1419171980   \n",
       "319    T327          0      31208.0            -1       86  1419171420   \n",
       "\n",
       "    DAY_TYPE  MISSING_DATA  YR  MON  DAY  HR  WK  STAND_HR  STAND_DAY  DAY_HR  \\\n",
       "0          A         False   0    0    2  11   2         9          4       1   \n",
       "1          A         False   0    0    2  11   2        44         15       1   \n",
       "2          A         False   0    0    2  11   2         9          4       1   \n",
       "3          A         False   0    0    2  11   2        41         13       1   \n",
       "4          A         False   0    0    2  11   2        15          5       1   \n",
       "..       ...           ...  ..  ...  ...  ..  ..       ...        ...     ...   \n",
       "315        A         False   0    4    3   8   4        60         32       7   \n",
       "316        A         False   0    4    3   8   4        40         31       7   \n",
       "317        A         False   0    4    3   8   4        60         32       7   \n",
       "318        A         False   0    4    3   8   4        60         32       7   \n",
       "319        A         False   0    4    3   8   4        60         32       7   \n",
       "\n",
       "     TOD  FROM_NOON  \n",
       "0      2          5  \n",
       "1      2          5  \n",
       "2      2          5  \n",
       "3      2          5  \n",
       "4      2          5  \n",
       "..   ...        ...  \n",
       "315    1          2  \n",
       "316    1          2  \n",
       "317    1          2  \n",
       "318    1          2  \n",
       "319    1          2  \n",
       "\n",
       "[320 rows x 18 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc296a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [87,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [106,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [125,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [64,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [144,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [11,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [30,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [83,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111/4072676577.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_111/4064483447.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0membedded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0membedded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Aggregate embeddings (e.g., using mean)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [140,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:702: indexSelectLargeIndex: block: [121,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    }
   ],
   "source": [
    "preds = model(X_test.long()).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': preds})\n",
    "\n",
    "final_preds.to_csv('test_preds.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac8bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
