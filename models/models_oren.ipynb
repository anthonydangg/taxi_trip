{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63e3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe27888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8763734",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07939271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only 10 rows with missing data/ maybe ok to just drop\n",
    "data[data['MISSING_DATA'] == True].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2774d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyline_to_trip_duration(polyline):\n",
    "    return max(polyline.count(\"[\") - 2, 0) * 15 #subtracting 2 because one is for the opening bracket?\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "data[\"LEN\"] = data[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9790a30",
   "metadata": {},
   "source": [
    "**Making time columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8332458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(x):\n",
    "    dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "    return dt.year, dt.month, dt.day, dt.hour, dt.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc43300",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f130faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(data, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d4a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data  #clearing memory since this dataframe is now redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04654645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              total        used        free      shared  buff/cache   available',\n",
       " 'Mem:         385381       37849      104195          92      243336      346327',\n",
       " 'Swap:             0           0           0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%system free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5c61",
   "metadata": {},
   "source": [
    "**Add interaction terms** ?\n",
    "- day * hour (both categorical)\n",
    "- hour * origin stand\n",
    "- day_type * origin stand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c7631",
   "metadata": {},
   "source": [
    "## Data engineering\n",
    "- Should all the features be categorical? Seems wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ae48df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n",
       "       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LEN', 'YR', 'MON',\n",
       "       'DAY', 'HR', 'WK'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns\n",
    "#independent variables are: call_type, origin_call, origin_stand, yr, mon, day, hr, week\n",
    "#all categorical?\n",
    "\n",
    "# most origin call are NaN (~78% of training data). probably drop, not sure how we'd infer it\n",
    "# about 52% of origin_stand are NaN. should we impute?\n",
    "#no NaN in call_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "864c8add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5282633621413164"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['ORIGIN_STAND'].isna()].shape[0] / train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d961ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']] #add back in call_type once we fix that\n",
    "Y_train = train[\"LEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7d9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "##why is this not working?\n",
    "#X_train['CALL_TYPE'] = X_train['CALL_TYPE'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104f2d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #probably want to do an embedding instead\n",
    "# encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# # Fit the encoder on the training data\n",
    "# encoder.fit(X_train[X_train.columns])\n",
    "# X_train_OH = encoder.transform(X_train[X_train.columns])\n",
    "\n",
    "# X_train = torch.Tensor(X_train_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "696322fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TRIP_ID', 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID',\n",
       "       'TIMESTAMP', 'DAY_TYPE', 'MISSING_DATA', 'POLYLINE', 'LEN', 'YR', 'MON',\n",
       "       'DAY', 'HR', 'WK'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efc5c006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CALL_TYPE', 'ORIGIN_STAND', 'TIMESTAMP', 'YR', 'MON', 'DAY', 'HR',\n",
      "       'WK'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, dropped_columns):\n",
    "        self.data = dataframe.drop(columns=dropped_columns)#.values\n",
    "        self.targets = dataframe[\"LEN\"].astype(float).values\n",
    "        print(self.data.columns)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        print(self.data[idx])\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataset = CustomDataset(train, dropped_columns=['LEN', 'POLYLINE', \n",
    "                                                'MISSING_DATA', 'TRIP_ID', 'ORIGIN_CALL',\n",
    "                                               \"DAY_TYPE\", 'TAXI_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7e5d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PandasDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         dataframe = dataframe[['CALL_TYPE', 'ORIGIN_STAND', 'YR', 'MON', 'DAY', 'HR',\n",
    "#        'WK']]\n",
    "#         dataframe['CALL_TYPE'] = dataframe['CALL_TYPE']\n",
    "#         self.data = dataframe[['CALL_TYPE', 'ORIGIN_STAND', 'YR', 'MON', 'DAY', 'HR',\n",
    "#        'WK']]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data.iloc[idx, :]\n",
    "#         print((x.values))\n",
    "#         return torch.from_numpy(x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff59d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.targets = dataframe[\"LEN\"].astype(float).values\n",
    "        self.data = dataframe[['CALL_TYPE', 'ORIGIN_STAND', 'YR', 'MON', 'DAY', 'HR', 'WK']]\n",
    "        self.call_type_mapping = {'A': 0, 'B': 1, 'C': 2}\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data.iloc[idx, :].copy()\n",
    "        x['CALL_TYPE'] = self.call_type_mapping[x['CALL_TYPE']]\n",
    "        \n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "     \n",
    "        return torch.from_numpy(x.values.astype(np.float32)), y\n",
    "\n",
    "dataset = PandasDataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51da7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = torch.utils.data.tensorDataset(X_train, torch.from_numpy(Y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d070c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70e5c6",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac60397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, num_call_types, num_origin_stands, embedding_dim, hidden_size):\n",
    "        super(BasicModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.call_type_embedding = nn.Embedding(num_call_types, embedding_dim)\n",
    "        self.origin_stand_embedding = nn.Embedding(num_origin_stands, embedding_dim)\n",
    "        \n",
    "        # Other layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + 4, hidden_size),  # 4 is for [hour, day, month, year]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, hour, day, month, year, call_type, origin_stand):\n",
    "        # Embedding lookup\n",
    "        call_type_embedded = self.call_type_embedding(call_type)\n",
    "        origin_stand_embedded = self.origin_stand_embedding(origin_stand)\n",
    "        \n",
    "        hour = hour.unsqueeze(1)\n",
    "        day = day.unsqueeze(1)\n",
    "        month = month.unsqueeze(1)\n",
    "        year = year.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        features = torch.cat([hour, day, month, year, call_type_embedded, origin_stand_embedded], dim=1)\n",
    "        \n",
    "        # Pass through the encoder\n",
    "        output = self.encoder(features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "faee631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure about input size\n",
    "num_call_types = train['CALL_TYPE'].nunique() \n",
    "num_origin_stands = train['ORIGIN_STAND'].nunique() \n",
    "embedding_dim = 10 #arbitrary\n",
    "hidden_size = 64 #arbitrary\n",
    "\n",
    "model = BasicModel(num_call_types, \n",
    "                   num_origin_stands, embedding_dim, \n",
    "                   hidden_size).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ac343b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x24 and 14x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_137/3857443130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mweek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#week is 7th col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_137/2959367027.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hour, day, month, year, call_type, origin_stand)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Pass through the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x24 and 14x64)"
     ]
    }
   ],
   "source": [
    "#deep network (not lstm)\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        \n",
    "        ['CALL_TYPE', 'ORIGIN_STAND', 'YR', 'MON', 'DAY', 'HR',\n",
    "       'WK']\n",
    "        \n",
    "        call_type = x_batch[:, 0].long()  # hour is the first column (index 0)\n",
    "        origin_stand = x_batch[:, 1].long()  # Assuming day is the second column (index 1)\n",
    "        year = x_batch[:, 2]  # Assuming month is the third column (index 2)\n",
    "        month = x_batch[:, 3]  # Assuming year is the fourth column (index 3)\n",
    "        day = x_batch[:, 4]  # Assuming call_type is the fifth column (index 4)\n",
    "        hour = x_batch[:, 5]  # Assuming origin_stand is the sixth column (index 5)\n",
    "        week = x_batch[:, 6] #week is 7th col\n",
    "\n",
    "        y_batch_pred = model(hour, day, month, year, call_type, origin_stand)\n",
    "\n",
    "\n",
    "        loss = loss_fn(y_batch_pred[0], y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} loss: {running_loss/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebeb43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 1 loss: 474716.41855863854\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 2 loss: 474706.96882588905\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 3 loss: 474695.4468317422\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 4 loss: 474669.8388911181\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 5 loss: 474651.3308392597\n"
     ]
    }
   ],
   "source": [
    "hidden_state = (torch.zeros(1, hidden_size).to(device),\n",
    "                torch.zeros(1, hidden_size).to(device))\n",
    "\n",
    "# https://stackoverflow.com/questions/51735001/how-to-include-batch-size-in-pytorch-basic-example\n",
    "for epoch in range(5):\n",
    "    \n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "\n",
    "    # Loop over batches in an epoch using DataLoader\n",
    "    for id_batch, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        y_batch_pred = model(x_batch, hidden_state)\n",
    "\n",
    "        loss = loss_fn(y_batch_pred[0], y_batch.float())\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "\n",
    "        # backwards steps\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} loss: {running_loss/n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53a492c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = valid[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "\n",
    "X_valid_OH = encoder.transform(X_valid[X_valid.columns])\n",
    "\n",
    "X_valid = torch.Tensor(X_valid_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe6ec6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = (torch.zeros(1, hidden_size).to(device),\n",
    "                torch.zeros(1, hidden_size).to(device))\n",
    "valid_preds = model(X_valid)#[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "649f2786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[711.6207],\n",
       "        [708.5093],\n",
       "        [714.1364],\n",
       "        ...,\n",
       "        [714.6372],\n",
       "        [719.8740],\n",
       "        [717.0795]], device='cuda:0', grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e663f",
   "metadata": {},
   "source": [
    "## Prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3993752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc00fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = (\n",
    "    test_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1edbdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "842e0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_OH = encoder.transform(X_test[X_test.columns])\n",
    "\n",
    "X_test = torch.Tensor(X_test_OH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95fee367",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = (torch.zeros(1, hidden_size).to(device),\n",
    "                torch.zeros(1, hidden_size).to(device))\n",
    "preds = model(X_test).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67208289",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame({'TRIP_ID': test_data['TRIP_ID'], 'TRAVEL_TIME': preds.cpu().detach()})\n",
    "\n",
    "final_preds.to_csv('test_preds.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab6213",
   "metadata": {},
   "source": [
    "# Old code, can ignore\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31733b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263863"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction[train_fraction['ORIGIN_STAND'].isna()].shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2464fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = train.sample(500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50de5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_fraction[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "y = train_fraction['LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0ba5008",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_Y = torch.tensor(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d54f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X = torch.from_numpy(X.values.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80160ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.nn.Sequential(\n",
    "torch.nn.Linear(6, 1))\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer (this will perform your parameter updates use)\n",
    "lr = 5e-3\n",
    "opt = torch.optim.SGD(model_1.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fdadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train.sample(500_000)\n",
    "X_test_pd = test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", 'ORIGIN_STAND']]\n",
    "y_test_pd = test['LEN']\n",
    "\n",
    "y_test = torch.tensor(y_test_pd.values)\n",
    "X_test = torch.tensor(X_test_pd.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orenciolli/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([500000])) that is different to the input size (torch.Size([500000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "train_err = []\n",
    "test_err = []\n",
    "parameters = []\n",
    "for i in range(100):\n",
    "    model_1.train()\n",
    "  \n",
    "    y_pred = model_1(full_X) # Compute model outputs\n",
    "    loss = loss_fn(y_pred, full_Y) # Compute MSE\n",
    "    opt.zero_grad() # Must reset the gradients every step. Otherwise, gradients from previous iterations would cause interference!!!\n",
    "    loss.backward() # Compute gradients of all parameters (our model) with respect to our computed loss value (a singular value)\n",
    "    opt.step() # One gradient step\n",
    "  \n",
    "    train_err.append(loss.item())\n",
    "  \n",
    "    model_1.eval()\n",
    "    with torch.no_grad():\n",
    "        test_err.append(loss_fn(model_1(X_test), y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b674c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "model_1(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cecf3",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6237bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fraction['POLYLINE'] = train_fraction['POLYLINE'].apply(lambda x: np.array(eval(x)).astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692e88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_fraction.groupby('ORIGIN_STAND').mean(numeric_only = True).plot(kind = 'bar', y = 'TRIP_LENGTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3907e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['ORIGIN_STAND'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca3d86",
   "metadata": {},
   "source": [
    "Predicted important variables: polyline, interaction between timestamp and origin stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843ee5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    train_fraction[['CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TIMESTAMP', 'DAY_TYPE', 'POLYLINE']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7482dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826e758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_x = train_fraction['POLYLINE'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "025d0444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['POLYLINE'].iloc[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d770a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/8n__h2151hd3n3gtfckwb75w0000gn/T/ipykernel_28498/3306076192.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_type = np.array([train_fraction['POLYLINE'].iloc[0], train_fraction['POLYLINE'].iloc[1]])\n"
     ]
    }
   ],
   "source": [
    "test_type = np.array([train_fraction['POLYLINE'].iloc[0], train_fraction['POLYLINE'].iloc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f95381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    np.array([0.5, 1.0, 2.0], dtype=np.float16),\n",
    "    np.array([4.0, 6.0, 8.0], dtype=np.float16),\n",
    "])\n",
    "\n",
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c630dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.59509"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_fraction['POLYLINE'])[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eea24aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d = np.array([\n",
    "    np.array([\n",
    "        np.array([1, 2]),\n",
    "        np.array([2, 3])\n",
    "    ]),\n",
    "    np.array([\n",
    "        np.array([1, 2]),\n",
    "        np.array([2, 3])\n",
    "    ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb89d8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_d.shape#[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88d25ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.59509 , 41.146434])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fraction['POLYLINE'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5b8cac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [2, 3]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(three_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abebfde0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOLYLINE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.from_numpy(train_fraction['POLYLINE'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a9c78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOLYLINE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "torch.tensor(train_fraction['POLYLINE'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c5fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
